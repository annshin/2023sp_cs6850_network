{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, listdir, mkdir\n",
    "from os.path import join\n",
    "import os\n",
    "from torch_geometric.data import HeteroData\n",
    "import json\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# check if gpu is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENVIRONMENT VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../unarXiv\"\n",
    "YEARS = ['91', '92', '93', '94', '95', '96', '97', '98', '99', '00', '01', '02', '03', '04', '05', '06', '07', '08', '09', \n",
    "            '10', '11', '12', '13', '14', '15', '16', '17', '18', '19']\n",
    "CATEGORIES = [\"cs.AI\", \"cs.CL\", \"cs.CV\", \"cs.LG\", \"stat.ML\"]\n",
    "PAPERS_BY_CATEGORY = True\n",
    "AUTHOR_PAST_YEARS = 5\n",
    "PREDICT_FOR_YEARS = 4\n",
    "DATASET_LAST_YEAR = 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ma952/Sets/2023sp_cs6850_network/WMRRank'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# papers = read_jsonl(join(data_path, str(folder), files))\n",
    "# for paper in papers: \n",
    "#     metadata = paper.get('metadata', {})\n",
    "#     if metadata.get('authors_parsed'):\n",
    "#         print(metadata['authors_parsed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cited_paper_id, cited_paper_info in paper[\"bib_entries\"].items():\n",
    "#     if cited_paper_info.get('contained_arXiv_ids'):\n",
    "#         # print(len(cited_paper_info['contained_arXiv_ids']))\n",
    "#         cited_paper_ids.append(cited_paper_info['contained_arXiv_ids'][0].get('id')) # 1 Paper at a time \n",
    "#         # print(cited_paper_info['contained_arXiv_ids'][0].get('id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 ../unarXiv/00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 12/12 [00:47<00:00,  3.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01 ../unarXiv/01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 12/12 [00:51<00:00,  4.29s/it]\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "total_papers = 0\n",
    "authors = []\n",
    "papers = []\n",
    "paper_info = pd.DataFrame(columns=['paper_id', 'title', 'authors', 'cited_paper_ids'])\n",
    "for folder in listdir(data_path):\n",
    "    if folder in YEARS:\n",
    "        print(folder, os.path.join(data_path, str(folder)))\n",
    "        if folder[0] == \"9\":\n",
    "            year = \"19\" + folder\n",
    "        else:\n",
    "            year = \"20\" + folder\n",
    "        for files in tqdm(listdir(join(data_path, str(folder)))):\n",
    "            papers_read = read_jsonl(join(data_path, str(folder), files))\n",
    "            total_papers += len(papers_read)\n",
    "            for paper in papers_read:\n",
    "                \n",
    "                cited_paper_ids = []\n",
    "                author_names = []\n",
    "                metadata = paper.get('metadata', {})\n",
    "                \n",
    "                if metadata.get('title') and metadata.get('authors_parsed'):\n",
    "                    \n",
    "                    paper_title = metadata['title']\n",
    "                    paper_id = paper[\"paper_id\"]\n",
    "                    found_match = (not PAPERS_BY_CATEGORY)\n",
    "                    if metadata.get('categories') and PAPERS_BY_CATEGORY:\n",
    "                        categories = metadata['categories'].split(' ')\n",
    "                        found_match = any([cat in CATEGORIES for cat in categories])\n",
    "                    \n",
    "                    if found_match:\n",
    "                        \n",
    "                        papers.append(paper_id) # 1 Paper\n",
    "                        \n",
    "                        for cited_paper_id, cited_paper_info in paper[\"bib_entries\"].items():\n",
    "                            if 'ids' not in cited_paper_info:\n",
    "                                continue\n",
    "                            elif 'arxiv_id' not in cited_paper_info['ids']:\n",
    "                                continue\n",
    "                            else:\n",
    "                                ref_paper_id = cited_paper_info['ids']['arxiv_id']\n",
    "                                if ref_paper_id == '':\n",
    "                                    continue\n",
    "                                cited_paper_ids.append(ref_paper_id)\n",
    "                        papers.extend(cited_paper_ids)\n",
    "                        \n",
    "                        if metadata.get('authors_parsed'):\n",
    "                            \n",
    "                            authors_parsed = metadata['authors_parsed']\n",
    "                            \n",
    "                            # Consider only top 5 authors\n",
    "                            for author in authors_parsed[:5]:\n",
    "                                author = author[:2]\n",
    "                                author_parsed = \" \".join(author)\n",
    "                                author_names.append(author_parsed)\n",
    "                                \n",
    "                            author_names = list(set(author_names))\n",
    "                            authors.extend(author_names) # Multiple authors at a time\n",
    "                            \n",
    "                        paper_info = paper_info.append({'paper_id': paper_id, 'title': paper_title, 'authors': author_names, 'cited_paper_ids': cited_paper_ids, 'year': year}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_pd = pd.DataFrame(authors)\n",
    "authors_pd = authors_pd.drop_duplicates()\n",
    "papers_pd = pd.DataFrame(papers)\n",
    "papers_pd = papers_pd.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_info.to_csv('paper_info.csv', index=False)\n",
    "papers_pd.to_csv('papers.csv', index=False, header=False)\n",
    "authors_pd.to_csv('authors.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.0'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         0\n",
      "0     Crutchfield James P.\n",
      "1    Shalizi Cosma Rohilla\n",
      "2          O'Rourke Joseph\n",
      "3             Schlei B. R.\n",
      "4                Prasad L.\n",
      "..                     ...\n",
      "595             Wets Geert\n",
      "596        Swinnen Gilbert\n",
      "597              Brijs Tom\n",
      "598    Bussche Jan Van den\n",
      "602          Geerts Floris\n",
      "\n",
      "[379 rows x 1 columns]\n",
      "              0\n",
      "0    cs/0001027\n",
      "1    cs/0001025\n",
      "2    cs/0001024\n",
      "3    cs/0001023\n",
      "4    cs/0001022\n",
      "..          ...\n",
      "353  cs/0112011\n",
      "354  cs/0112007\n",
      "355  cs/0112005\n",
      "357  cs/0112004\n",
      "358  cs/0112003\n",
      "\n",
      "[319 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(authors_pd)\n",
    "print(papers_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "612\n"
     ]
    }
   ],
   "source": [
    "print(len(authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,v in paper_info.iterrows():\n",
    "#     print(v['paper_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of paper IDs to consecutive values:\n",
      "==========================================\n",
      "      paperId  mappedID\n",
      "0  cs/0001027         0\n",
      "1  cs/0001025         1\n",
      "2  cs/0001024         2\n",
      "3  cs/0001023         3\n",
      "4  cs/0001022         4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_paper_id = papers_pd[0].unique()\n",
    "unique_paper_id = pd.DataFrame(data={\n",
    "    'paperId': unique_paper_id,\n",
    "    'mappedID': pd.RangeIndex(len(unique_paper_id)),\n",
    "})\n",
    "print(\"Mapping of paper IDs to consecutive values:\")\n",
    "print(\"==========================================\")\n",
    "print(unique_paper_id.head())\n",
    "print()\n",
    "unique_paper_id.to_csv('unique_paper_id.csv', index=False, header=False)\n",
    "\n",
    "# for i,v in unique_paper_id.iterrows():\n",
    "#     print(v)\n",
    "\n",
    "# unique_author_id = authors_pd[0].unique()\n",
    "# unique_author_id = pd.DataFrame(data={\n",
    "#     'authorId': unique_author_id,\n",
    "#     'mappedID': pd.RangeIndex(len(unique_author_id)),\n",
    "# })\n",
    "# print(\"Mapping of author IDs to consecutive values, remember each author in a year is treated as a new entity:\")\n",
    "# print(\"==========================================\")\n",
    "# print(unique_author_id.head())\n",
    "# print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GENERATING THE HETEROGENOUS GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_info_2 = paper_info.explode('authors')\n",
    "paper_exploded = paper_info_2.reset_index(drop=True)\n",
    "paper_exploded['position'] = paper_exploded.groupby(['paper_id', 'title']).cumcount() + 1\n",
    "paper_info_2 = paper_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique author entities:  425\n"
     ]
    }
   ],
   "source": [
    "# Computing unique total entities by treating authors in new years as seperate entities\n",
    "paper_info_2['authors_year'] = paper_info_2['authors'].astype(str) + '_' + paper_info_2['year'].astype(str)\n",
    "\n",
    "total_author_entities = len(paper_info_2['authors_year'].unique())\n",
    "unique_author_year_id = paper_info_2['authors_year'].unique()\n",
    "\n",
    "unique_author_year_id = pd.DataFrame(data={\n",
    "    'authorId': unique_author_year_id,\n",
    "    'mappedID': pd.RangeIndex(total_author_entities),\n",
    "})\n",
    "\n",
    "print(\"Total number of unique author entities: \", total_author_entities)\n",
    "\n",
    "# Using this to create the edges of the heterograph\n",
    "paper_info_3 = paper_info.explode('cited_paper_ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_pd_list = unique_author_year_id['authorId'].tolist()\n",
    "papers_pd_list = unique_paper_id['paperId'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Edge indices:\n",
      "=============\n",
      "Final edge indices pointing from Paper to author:\n",
      "------------------------\n",
      "tensor([[  0.,   0.,   1.,  ..., 318., 318., 318.],\n",
      "        [  0.,   1.,   2.,  ..., 287., 288., 285.]])\n",
      "\n",
      "Final edge indices pointing from Paper to cited papers:\n",
      "------------------------\n",
      "tensor([[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,  11.,\n",
      "          13.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,  22.,  23.,  24.,  25.,\n",
      "          26.,  27.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
      "          39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,  50.,\n",
      "          51.,  53.,  54.,  55.,  56.,  57.,  58.,  59.,  60.,  60.,  60.,  60.,\n",
      "          60.,  60.,  60.,  60.,  68.,  69.,  69.,  72.,  73.,  73.,  73.,  74.,\n",
      "          75.,  76.,  78.,  79.,  80.,  82.,  83.,  84.,  85.,  86.,  87.,  88.,\n",
      "          88.,  91.,  92.,  93.,  94.,  95.,  96.,  98.,  97.,  99., 100., 101.,\n",
      "         103., 104., 105., 106., 107., 108., 109., 110., 112., 113., 114., 115.,\n",
      "         117., 118., 119., 119., 122., 123., 124., 125., 126., 127., 127., 128.,\n",
      "         129., 131., 131., 131., 132., 133., 135., 136., 137., 138., 139., 140.,\n",
      "         141., 142., 143., 143., 146., 147., 148., 149., 150., 152., 153., 154.,\n",
      "         155., 156., 157., 159., 160., 161., 162., 164., 165., 166., 167., 168.,\n",
      "         169., 170., 171., 172., 173., 174., 175., 176., 178., 178., 178., 178.,\n",
      "         183., 184., 185., 186., 187., 188., 189., 190., 190., 190., 194., 195.,\n",
      "         196., 197., 198., 199., 200., 201., 202., 203., 204., 205., 206., 207.,\n",
      "         208., 209., 210., 211., 212., 212., 215., 216., 217., 218., 219., 220.,\n",
      "         221., 222., 223., 224., 225., 226., 227., 229., 230., 231., 232., 233.,\n",
      "         234., 235., 111., 236., 237., 238., 239., 240., 241., 242., 243., 244.,\n",
      "         245., 246., 247., 248., 249., 250., 251., 252., 253., 254., 255., 256.,\n",
      "         257., 258., 258., 260., 261., 262., 263., 264., 265., 266., 267., 268.,\n",
      "         269., 270., 270., 272., 273., 274., 275., 276., 278., 278., 280., 281.,\n",
      "         282., 283., 284., 285., 286., 287., 288., 289., 289., 291., 291., 292.,\n",
      "         293., 294., 295., 296., 297., 298., 299., 300., 301., 302., 303., 304.,\n",
      "         305., 305., 308., 310., 311., 312., 313., 314., 315., 316., 317., 318.],\n",
      "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  12.,\n",
      "          14.,  nan,  nan,  17.,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
      "          nan,  28.,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
      "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
      "          52.,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  61.,  62.,  63.,  64.,\n",
      "          65.,  65.,  66.,  67.,  nan,  70.,  71.,  nan,  64.,  65.,  66.,  nan,\n",
      "          nan,  77.,  nan,  nan,  81.,  nan,  nan,  nan,  nan,  nan,  nan,  89.,\n",
      "          90.,  nan,  nan,  nan,  nan,  nan,  97.,  nan,  nan,  97.,  97., 102.,\n",
      "          nan,  nan,  nan,  nan,  nan,  nan,  nan, 111.,  nan,  nan,  nan, 116.,\n",
      "          nan,  nan, 120., 121.,  nan, 123.,  nan,  nan,  nan,  64.,  65.,  nan,\n",
      "         130.,  70.,  64.,  65.,  nan, 134., 134.,  nan,  nan,  nan,  nan,  nan,\n",
      "          nan,  nan, 144., 145., 144.,  nan,  nan, 121., 151.,  nan,  nan,  nan,\n",
      "          nan, 144., 158., 121.,  nan,  nan, 163.,  nan,  nan,  nan,  nan,  nan,\n",
      "          nan,  nan,  nan,  nan, 121.,  nan,  nan, 177., 179., 180., 181., 182.,\n",
      "          nan, 134.,  nan,  nan,  nan,  nan,  nan, 191., 192., 193.,  nan,  nan,\n",
      "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
      "          nan,  nan,  nan,  nan, 213., 214.,  nan,  nan,  nan,  nan,  nan,  nan,\n",
      "          nan,  nan,  nan,  nan,  nan,  nan, 228.,  nan,  nan,  nan,  nan,  nan,\n",
      "          nan,  nan, 117.,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan, 163.,\n",
      "          nan,  nan,  nan, 130., 121., 121.,  nan,  nan,  nan,  nan, 121.,  nan,\n",
      "          nan, 259.,  97.,  nan,  nan,  nan,  nan, 265.,  nan,  nan,  nan,  nan,\n",
      "          nan, 271., 121.,  nan,  nan,  nan,  nan, 277., 279., 280., 279.,  nan,\n",
      "          nan,  nan,  nan,  nan, 130.,  nan,  nan, 228., 290.,  64.,  65.,  nan,\n",
      "          nan,  nan,  nan,  nan,  nan, 279.,  nan,  nan,  nan,  nan,  nan,  nan,\n",
      "         306., 307., 309., 121.,  nan,  nan,  nan,  nan,  nan, 225.,  nan,  nan]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(unique_paper_id)\n",
    "paper_id_info = pd.merge(paper_info_2['paper_id'], unique_paper_id, left_on='paper_id', right_on='paperId', how='left')\n",
    "# print(paper_id_info)\n",
    "paper_id_info = torch.from_numpy(paper_id_info['mappedID'].values)\n",
    "\n",
    "paper_author_year_info = pd.merge(paper_info_2['authors_year'], unique_author_year_id, left_on='authors_year', right_on='authorId', how='left')\n",
    "paper_author_year_info = torch.from_numpy(paper_author_year_info['mappedID'].values)\n",
    "\n",
    "paper_author_year_weight = torch.from_numpy(paper_info_2['position'].values)\n",
    "year_weight = torch.from_numpy(np.array(list(map(int,paper_info_2['year'].values))))\n",
    "\n",
    "attr_weights = torch.stack([paper_author_year_weight.float(), year_weight.float()], dim = 0)\n",
    "# for i in range(attr_weights[0].shape[0]):\n",
    "#     print(attr_weights[0][i])\n",
    "\n",
    "# Construct the paper-author edges:\n",
    "paper_author_edge_index = torch.stack([paper_id_info.float(), paper_author_year_info.float()], dim=0)\n",
    "\n",
    "paper_id_info = pd.merge(paper_info_3['paper_id'], unique_paper_id, left_on='paper_id', right_on='paperId', how='left')\n",
    "paper_id_info = torch.from_numpy(paper_id_info['mappedID'].values)\n",
    "\n",
    "paper_cite_info = pd.merge(paper_info_3['cited_paper_ids'], unique_paper_id, left_on='cited_paper_ids', right_on='paperId', how='left')\n",
    "paper_cite_info = torch.from_numpy(paper_cite_info['mappedID'].values)\n",
    "\n",
    "# Construct the paper-paper edges:\n",
    "paper_cite_edge_index = torch.stack([paper_id_info.float(), paper_cite_info.float()], dim=0)\n",
    "\n",
    "print()\n",
    "print(\"Edge indices:\")\n",
    "print(\"=============\")\n",
    "print(\"Final edge indices pointing from Paper to author:\")\n",
    "print(\"------------------------\")\n",
    "print(paper_author_edge_index)\n",
    "print()\n",
    "print(\"Final edge indices pointing from Paper to cited papers:\")\n",
    "print(\"------------------------\")\n",
    "print(paper_cite_edge_index)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import remove_self_loops\n",
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "print(torch_geometric.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001b[1mpaper\u001b[0m={ node_id=[319] },\n",
      "  \u001b[1mauthor\u001b[0m={ node_id=[425] },\n",
      "  \u001b[1m(paper, written_by, author)\u001b[0m={\n",
      "    edge_index=[2, 612],\n",
      "    edge_attr=[2, 612]\n",
      "  },\n",
      "  \u001b[1m(paper, cites, paper)\u001b[0m={ edge_index=[2, 300] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "data = HeteroData()\n",
    "\n",
    "data[\"paper\"].node_id = torch.arange(len(unique_paper_id)).float()\n",
    "data[\"author\"].node_id = torch.arange(len(unique_author_year_id)).float()\n",
    "\n",
    "data[\"paper\", \"written_by\", \"author\"].edge_index = paper_author_edge_index\n",
    "data[\"paper\", \"cites\", \"paper\"].edge_index = paper_cite_edge_index\n",
    "data[\"paper\", \"written_by\", \"author\"].edge_attr = attr_weights\n",
    "\n",
    "print(data)\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WMR RANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 1., 1., 2., 1., 2., 1., 2., 1., 2., 1., 1., 2., 1., 1., 2., 3.,\n",
       "        1., 2., 1., 1., 1., 2., 3., 1., 2., 3., 1., 2., 1., 2., 1., 2., 3., 1.,\n",
       "        1., 2., 1., 1., 2., 1., 1., 1., 2., 1., 2., 1., 2., 3., 1., 2., 3., 4.,\n",
       "        1., 2., 1., 2., 3., 1., 2., 1., 2., 1., 1., 1., 2., 1., 1., 2., 3., 1.,\n",
       "        2., 3., 1., 2., 3., 4., 1., 1., 2., 3., 1., 2., 1., 2., 1., 2., 3., 1.,\n",
       "        2., 3., 4., 1., 2., 3., 4., 5., 1., 1., 2., 3., 1., 2., 3., 1., 2., 1.,\n",
       "        2., 1., 2., 1., 2., 1., 2., 3., 1., 1., 2., 3., 1., 1., 1., 2., 3., 1.,\n",
       "        1., 1., 1., 1., 2., 3., 1., 2., 3., 4., 1., 2., 1., 1., 1., 2., 1., 2.,\n",
       "        3., 1., 1., 1., 2., 3., 1., 1., 1., 1., 1., 1., 2., 1., 2., 3., 1., 1.,\n",
       "        2., 3., 1., 2., 3., 1., 2., 1., 1., 2., 1., 2., 1., 2., 3., 1., 2., 3.,\n",
       "        1., 2., 1., 2., 3., 4., 1., 2., 1., 2., 3., 4., 1., 2., 3., 1., 1., 1.,\n",
       "        2., 3., 4., 5., 1., 2., 3., 4., 5., 1., 2., 3., 1., 1., 2., 1., 2., 1.,\n",
       "        2., 1., 2., 1., 2., 3., 1., 1., 2., 3., 4., 1., 2., 3., 1., 1., 2., 1.,\n",
       "        2., 3., 1., 2., 3., 4., 5., 1., 2., 1., 1., 2., 3., 1., 2., 3., 1., 2.,\n",
       "        3., 4., 1., 2., 1., 2., 3., 4., 1., 1., 2., 3., 4., 1., 2., 3., 1., 2.,\n",
       "        3., 4., 5., 1., 2., 3., 4., 1., 2., 3., 4., 1., 2., 1., 2., 3., 4., 5.,\n",
       "        1., 2., 1., 2., 1., 1., 2., 1., 2., 3., 4., 1., 2., 1., 2., 1., 2., 1.,\n",
       "        2., 3., 1., 1., 1., 2., 3., 4., 1., 2., 1., 1., 2., 1., 2., 3., 1., 1.,\n",
       "        2., 1., 2., 1., 2., 1., 1., 2., 1., 1., 2., 1., 2., 3., 1., 1., 2., 3.,\n",
       "        1., 2., 3., 1., 2., 1., 1., 2., 3., 4., 1., 1., 2., 1., 2., 3., 1., 2.,\n",
       "        3., 4., 5., 1., 2., 1., 2., 3., 1., 2., 3., 1., 1., 1., 2., 1., 1., 2.,\n",
       "        1., 2., 1., 2., 1., 2., 3., 1., 2., 1., 2., 1., 2., 3., 4., 1., 1., 1.,\n",
       "        1., 2., 1., 1., 2., 1., 1., 2., 3., 4., 5., 1., 2., 3., 4., 5., 1., 2.,\n",
       "        3., 4., 1., 2., 3., 4., 1., 2., 3., 1., 2., 3., 4., 5., 1., 2., 3., 1.,\n",
       "        2., 1., 1., 1., 1., 1., 1., 2., 3., 4., 1., 2., 3., 1., 2., 3., 4., 1.,\n",
       "        2., 1., 2., 1., 2., 3., 1., 2., 3., 4., 1., 1., 2., 1., 1., 2., 1., 1.,\n",
       "        1., 2., 3., 1., 2., 1., 2., 1., 2., 3., 4., 5., 1., 1., 1., 2., 1., 1.,\n",
       "        2., 1., 1., 2., 1., 2., 1., 1., 1., 1., 1., 2., 1., 2., 1., 2., 1., 2.,\n",
       "        3., 1., 2., 1., 2., 3., 4., 5., 1., 2., 3., 1., 1., 2., 3., 4., 5., 1.,\n",
       "        2., 3., 1., 2., 1., 2., 1., 1., 2., 3., 4., 5., 1., 1., 2., 3., 4., 1.,\n",
       "        1., 2., 1., 2., 3., 1., 1., 2., 1., 2., 3., 4., 1., 2., 3., 4., 1., 2.,\n",
       "        1., 1., 1., 2., 1., 1., 2., 3., 1., 2., 1., 2., 3., 1., 2., 3., 1., 2.,\n",
       "        3., 1., 1., 1., 2., 3., 1., 2., 3., 4., 5., 1., 2., 1., 2., 1., 1., 1.,\n",
       "        2., 3., 4., 5., 1., 2., 1., 2., 3., 1., 2., 1., 2., 3., 1., 2., 3., 4.],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"paper\", \"written_by\", \"author\"].edge_attr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "MU = { 1991: 0.0, 1992: 0.0, 1993: 0.0, 1994: 0.0, 1995: 0.0, 1996: 0.0, 1997: 0.0, 1998: 0.0, 1999: 0.0,\n",
    "       2000: 0.1, 2001: 0.1, 2002: 0.1, 2003: 0.1, 2004: 0.1, 2005: 0.1, 2006: 0.2, 2007: 0.2, 2008: 0.2, \n",
    "       2009: 0.2, 2010: 0.3, 2011: 0.4, 2012: 0.5, 2013: 0.5, 2014: 0.5, 2015: 0.5, 2016: 0.6, 2017: 0.6, \n",
    "       2018: 0.7, 2019: 0.8, 2020: 0.8, 2021: 0.8, 2022: 0.8, 2023: 0.9 }\n",
    "\n",
    "class WMR_Rank:\n",
    "    \n",
    "    def __init__(self, heteroData, device):\n",
    "        self.checkpoint_path = './checkpoints/'\n",
    "        self.device = device\n",
    "        self.pp_edge_index = heteroData['paper', 'cites', 'paper'].edge_index.to(device).to(torch.float64)\n",
    "        self.pa_edge_index = heteroData['paper', 'written_by', 'author'].edge_index.to(device).to(torch.float64)\n",
    "        self.pa_edge_attr = heteroData['paper', 'written_by', 'author'].edge_attr.to(device).to(torch.float64)\n",
    "        \n",
    "        \n",
    "    def get_mapped_author_year(self, author_year):\n",
    "        return torch.tensor(unique_author_year_id[unique_author_year_id['authorId'] == author_year]['mappedID'].values[0]).to(self.device)\n",
    "    \n",
    "    def get_mapped_paper(self, paper):\n",
    "        return torch.tensor(unique_paper_id[unique_paper_id['paperId'] == paper]['mappedID'].values[0]).to(self.device)\n",
    "    \n",
    "    def get_unmapped_author_year(self, author_year):\n",
    "        return torch.tensor(unique_author_year_id[unique_author_year_id['mappedID'] == author_year]['authorId'].values[0]).to(self.device)\n",
    "    \n",
    "    def get_unmapped_paper(self, paper):\n",
    "        return torch.tensor(unique_paper_id[unique_paper_id['mappedID'] == paper]['paperId'].values[0]).to(self.device)\n",
    "    \n",
    "    def order(self, paper, author, tomap = False):\n",
    "        if tomap:\n",
    "            paper = self.get_mapped_paper(paper)\n",
    "            author = self.get_mapped_author_year(author)\n",
    "        result = self.pa_edge_attr[0][(self.pa_edge_index[0] == paper) & (self.pa_edge_index[1] == author)]\n",
    "        return result.to(device)\n",
    "        \n",
    "    def pnai(self, AS, VA, author_year, ta):\n",
    "        author = '_'.join(author_year.split('_')[:-1])\n",
    "        current_year = int(author_year.split('_')[-1])\n",
    "        author_perf = torch.zeros(ta).type(torch.float64).to(device)\n",
    "        for i in range(ta):\n",
    "            author_pass = author + '_' + str(current_year - i - 1)\n",
    "            if author_pass in VA:\n",
    "#                 print(\"PNAI: \", author_pass)\n",
    "                author_perf[i] = AS[self.get_mapped_author_year(author_pass)]\n",
    "        return author_perf.to(device)   \n",
    "    \n",
    "    def find_papers_citing_papers_until_year(self, paper, year):\n",
    "        papers_cited_mapped = torch.unique(self.pp_edge_index[0][self.pp_edge_index[1] == paper])\n",
    "        years_citing_papers = torch.Tensor().to(device).to(torch.float64)\n",
    "        for i in papers_cited_mapped:\n",
    "            years_citing_papers = torch.cat((years_citing_papers, self.paper_published_year(i)))\n",
    "        years_citing_papers = years_citing_papers[years_citing_papers <= year]\n",
    "        return years_citing_papers.to(device)    \n",
    "    \n",
    "    def find_papers_citing_paper(self, paper):\n",
    "        papers_cited_mapped = torch.unique(self.pp_edge_index[0][self.pp_edge_index[1] == paper])\n",
    "        return papers_cited_mapped.to(device)\n",
    "    \n",
    "    def recent_papers_citing_paper(self, paper, publish_year, l):\n",
    "        device = self.device\n",
    "        citing_papers = self.find_papers_citing_paper(paper)\n",
    "        years_citing_papers = torch.Tensor().to(device).to(torch.float64)\n",
    "        for i in citing_papers:\n",
    "            years_citing_papers = torch.cat((years_citing_papers, self.paper_published_year(i)))\n",
    "        years_citing_papers, _ = torch.sort(years_citing_papers, descending = True)\n",
    "        \n",
    "        if (years_citing_papers.shape[0] < l):\n",
    "            extra_years = torch.full((l - years_citing_papers.shape[0], ), publish_year).to(device)\n",
    "            years_citing_papers = torch.cat((years_citing_papers, extra_years))\n",
    "        years_citing_papers = years_citing_papers[:l]\n",
    "        return years_citing_papers.to(device)\n",
    "    \n",
    "    def T_pi(self, paper, current_year, sigma1):\n",
    "        publish_year = int(self.paper_published_year(paper))\n",
    "        evaluation_year = current_year\n",
    "        l = int(evaluation_year) - publish_year\n",
    "        years_citing_papers = self.recent_papers_citing_paper(paper, publish_year, l)\n",
    "        years_citing_papers = years_citing_papers.to(torch.float64)\n",
    "        T_avg = torch.mean(years_citing_papers).to(torch.float64)\n",
    "        T_pi = torch.exp(-sigma1 * (current_year - T_avg))/ (current_year - T_avg)\n",
    "        return T_pi.to(device).to(torch.float64)\n",
    "    \n",
    "    def T_ai(self, author_year, sigma2, current_year, m = 10):\n",
    "        device = self.device\n",
    "        papers = self.get_all_papers_by_author_year(author_year)\n",
    "        citing_papers = torch.Tensor().to(device).to(torch.float64)\n",
    "        years_citing_papers = torch.Tensor().to(device).to(torch.float64)\n",
    "        citing_papers = self.pp_edge_index[0][torch.isin(self.pp_edge_index[1], papers)]\n",
    "        citing_papers = torch.unique(citing_papers)\n",
    "        for i in citing_papers:\n",
    "            years_citing_papers = torch.cat((years_citing_papers, self.paper_published_year(i)))\n",
    "        years_citing_papers, _ = torch.sort(years_citing_papers, descending = True)\n",
    "        year = float(author_year.split('_')[-1])\n",
    "        if years_citing_papers.shape[0] >= m:\n",
    "            years_citing_papers = years_citing_papers[:m]\n",
    "        elif (years_citing_papers.shape[0] < m):\n",
    "            extra_years = torch.full((m - years_citing_papers.shape[0], ), year).to(device).to(torch.float64)\n",
    "            years_citing_papers = torch.cat((years_citing_papers, extra_years))\n",
    "            \n",
    "        T_avg = torch.mean(years_citing_papers).to(device).to(torch.float64)\n",
    "        T_ai = torch.exp(-sigma2 * (current_year - T_avg))/ (current_year - T_avg)\n",
    "        return T_ai.to(device).to(torch.float64)    \n",
    "        \n",
    "    def paper_paper_edge_exists(self, paper_i, paper_j):\n",
    "        check = torch.logical_and((self.pp_edge_index[0] == paper_j), (self.pp_edge_index[1] == paper_i))\n",
    "        has_edge = torch.any(check)\n",
    "        if has_edge:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "  \n",
    "    def get_all_papers_by_author_year(self, author_year):\n",
    "        papers = torch.unique(self.pa_edge_index[0][self.pa_edge_index[1] == author_year])\n",
    "        return papers.to(device)\n",
    "    \n",
    "          \n",
    "#     def paper_author_edge_exists(self, paper, author):\n",
    "#         paper_mapped = unique_paper_id[unique_paper_id['paperId'] == paper]['mappedID'].values[0]\n",
    "#         author_mapped = unique_author_id[unique_author_id['authorId'] == author]['mappedID'].values[0]\n",
    "#         edge_index = self.heteroData['paper', 'written_by', 'author'].edge_index\n",
    "#         has_edge = torch.where((edge_index[0] == paper_mapped) & (edge_index[1] == author_mapped))    \n",
    "#         if len(has_edge[0]) > 0:\n",
    "#             return True\n",
    "#         else:\n",
    "#             return False\n",
    "      \n",
    "        \n",
    "#     def get_papers_cited_by_paper(self, paper):\n",
    "#         paper_mapped = unique_paper_id[unique_paper_id['paperId'] == paper]['mappedID'].values[0]\n",
    "#         papers = self.pp_edge_index[1][self.pp_edge_index[0] == paper_mapped]\n",
    "#         papers = papers.numpy().tolist()\n",
    "#         if paper_mapped in papers:\n",
    "#             papers.remove(paper_mapped)\n",
    "#         papers = unique_paper_id[unique_paper_id['mappedID'].isin(papers)]['paperId'].values\n",
    "#         papers = list(set(papers))\n",
    "#         return papers\n",
    "    \n",
    "    def get_all_authors_of_paper(self, paper):\n",
    "        authors = torch.unique(self.pa_edge_index[1][self.pa_edge_index[0] == paper])\n",
    "        return authors.to(device)\n",
    "    \n",
    "    def get_all_coauthors_and_citing_authors(self, author_year):\n",
    "        device = self.device\n",
    "        papers_by_author = self.get_all_papers_by_author_year(author_year)\n",
    "        coauthors = torch.Tensor().to(device).to(torch.float64)\n",
    "        authors_that_cited = torch.Tensor().to(device).to(torch.float64)\n",
    "        for i in papers_by_author:\n",
    "            coauthors_paper = self.get_all_authors_of_paper(i)\n",
    "            citing_papers = self.find_papers_citing_paper(i)\n",
    "            coauthors = torch.cat((coauthors, coauthors_paper))\n",
    "            for j in citing_papers:\n",
    "                authors_that_cited = torch.cat((authors_that_cited, self.get_all_authors_of_paper(j)))\n",
    "        final_authors = torch.cat((authors_that_cited, coauthors))\n",
    "        final_authors = torch.unique(final_authors)\n",
    "        final_authors = final_authors[final_authors != author_year]\n",
    "        return final_authors.to(device).to(torch.float64)\n",
    "        \n",
    "        \n",
    "    def W_ca_raw(self, author_m, author_n, paper_i, paper_j):\n",
    "        return (1./ (self.order(paper_i, author_m) * self.order(paper_j, author_n))).to(device).to(torch.float64)\n",
    "\n",
    "    def W_ca(self, author_m, author_n, paper_i, paper_j):\n",
    "        device = self.device\n",
    "        num = self.W_ca_raw(author_m, author_n, paper_i, paper_j)\n",
    "        authors_k = self.get_all_authors_of_paper(paper_i)\n",
    "        authors_l = self.get_all_authors_of_paper(paper_j)\n",
    "        \n",
    "        denom = torch.zeros(1).to(device).to(torch.float64)\n",
    "        for k in range(authors_k.shape[0]):\n",
    "            for l in range(authors_l.shape[0]):\n",
    "                if authors_k[k] != authors_l[l]:\n",
    "                    denom = denom + self.W_ca_raw(authors_k[k], authors_l[l], paper_i, paper_j)\n",
    "        if denom != 0.0:\n",
    "            return (num/denom).to(device)\n",
    "        else:\n",
    "            return torch.zeros(1).to(device).to(torch.float64)\n",
    "\n",
    "    def W_ca_total(self, author_m, author_n):\n",
    "        device = self.device\n",
    "        p_i = self.get_all_papers_by_author_year(author_m)\n",
    "        p_j = self.get_all_papers_by_author_year(author_n)\n",
    "        sum_final = torch.zeros(1).to(device).to(torch.float64)\n",
    "        for i in range(p_i.shape[0]):\n",
    "            for j in range(p_j.shape[0]):\n",
    "                if self.paper_paper_edge_exists(p_i[i], p_j[j]) and (p_i[i] != p_j[j]):\n",
    "                    sum_final = sum_final + self.W_ca(author_m, author_n, p_i[i], p_j[j])\n",
    "        return sum_final.to(device)\n",
    "    \n",
    "    def W_coa_raw(self, author_i, author_j, paper):\n",
    "#         print(\"Check for Order: \", self.pa_edge_index[1][self.pa_edge_index[0] == paper])\n",
    "#         print(\"Author_i: \", author_i)\n",
    "#         print(\"Paper: \", paper)\n",
    "#         print(\"Order: \", self.order(paper, author_i))\n",
    "        return (1./ (self.order(paper, author_i) * self.order(paper, author_j))).to(device).to(torch.float64)\n",
    "    \n",
    "    def W_coa(self, author_i, author_j, p):\n",
    "        denom = torch.zeros(1, device = self.device).to(torch.float64)\n",
    "        authors = self.get_all_authors_of_paper(p)\n",
    "        if authors.shape[0] > 1:\n",
    "            for i in range(authors.shape[0]):\n",
    "                for j in range(authors.shape[0]):\n",
    "                    if authors[i] != authors[j]:\n",
    "                        denom = denom + self.W_coa_raw(authors[i], authors[j], p)\n",
    "            if denom != 0.0:\n",
    "                return self.W_coa_raw(author_i, author_j, p)/denom\n",
    "        else:\n",
    "            return torch.zeros(1).to(device).to(torch.float64)\n",
    "    \n",
    "    def W_coa_total(self, author_i, author_j):\n",
    "        papers_i = torch.unique(self.pa_edge_index[0][(self.pa_edge_index[1] == author_i)])\n",
    "        papers_j = torch.unique(self.pa_edge_index[0][(self.pa_edge_index[1] == author_j)])\n",
    "        papers_cat, counts = torch.cat([papers_i, papers_j]).unique(return_counts=True)\n",
    "        papers = papers_cat[torch.where(counts.gt(1))]\n",
    "        papers = torch.unique(papers)\n",
    "        sum_final = torch.zeros(1, device = self.device).to(torch.float64)\n",
    "        for paper in papers:\n",
    "            sum_final = sum_final + self.W_coa(author_i, author_j, paper)\n",
    "        return sum_final\n",
    "    \n",
    "    def W_aa(self, author_i, author_j):\n",
    "        result = self.W_ca_total(author_i, author_j)\n",
    "        result_2 = self.W_coa_total(author_i, author_j)\n",
    "        return result + result_2\n",
    "    \n",
    "    def W_pa(self, papers_array, author_year):\n",
    "        device = self.device\n",
    "        num_authors = torch.sum(self.pa_edge_index[0] == papers_array).to(device).to(torch.float64)\n",
    "        position_author = self.order(papers_array, author_year)\n",
    "        return ((2.**(num_authors - position_author)) / ((2.**num_authors) - 1.0)).to(self.device)\n",
    "    \n",
    "    def W_pp(self, p_i, p_j):\n",
    "        if self.paper_paper_edge_exists(p_i, p_j):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def paper_published_year(self, papers):\n",
    "        return torch.unique(self.pa_edge_attr[1][self.pa_edge_index[0] == papers]).to(device)\n",
    "    \n",
    "    def weighted_citation_by_author(self, author):\n",
    "        device = self.device\n",
    "        papers = self.get_all_papers_by_author_year(author).to(device)\n",
    "        score = torch.tensor([0.0]).to(device).to(torch.float64)\n",
    "        for paper in papers: \n",
    "            citation_count = self.find_papers_citing_paper(paper).to(device).to(torch.float64)\n",
    "            score = score + self.W_pa(paper, author) * (citation_count.shape[0])\n",
    "        return score\n",
    "            \n",
    "    # alpha = 0.70, beta = 0.60, lmbda = 0.85,  sigma_1 = 0.5, sigma_2 = 1.0, mu = [0.2, 0.8, 0.2], VA = list of authors_pd, VP = list of papers_pd\n",
    "    def algo(self, current_year, VP, VA, alpha, beta, sigma1, sigma2, lmbda, epsilon, ta=2):\n",
    "        # Assumption\n",
    "        mu = MU\n",
    "        device = self.device\n",
    "        \n",
    "        alpha = alpha.to(device)\n",
    "        beta = beta.to(device)\n",
    "        sigma1 = sigma1.to(device)\n",
    "        sigma2 = sigma2.to(device)\n",
    "        lmbda = lmbda.to(device)\n",
    "        epsilon = epsilon.to(device)\n",
    "        \n",
    "        PS = torch.zeros(PREDICT_FOR_YEARS + 1, len(VP)).to(device) # Capped at 5 years\n",
    "        PS = PS.to(torch.float64)\n",
    "        PS[0] = (torch.ones(len(VP)) / len(VP)).to(torch.float64)\n",
    "        \n",
    "        AS = torch.zeros(PREDICT_FOR_YEARS + 1, len(VA)).to(device)\n",
    "        AS = AS.to(torch.float64)\n",
    "        AS[0] = ((torch.ones(len(VA)) / len(VA))).to(torch.float64)\n",
    "        \n",
    "        ground_truth_years_count = DATASET_LAST_YEAR - current_year\n",
    "        weighted_citation = torch.zeros(ground_truth_years_count, len(VA)).to(device).to(torch.float64)\n",
    "        \n",
    "        pas = torch.ones(len(VA)).to(device).to(torch.float64)\n",
    "        # Represents the current_year - 1 in terms of symbolic notation,  1 represents first analysis year\n",
    "        t = -1         \n",
    "        delta = torch.tensor([2 * epsilon]).to(torch.float64).to(device)\n",
    "        while delta > epsilon and tqdm(t < (PREDICT_FOR_YEARS - 1)):\n",
    "            # First iteration 0\n",
    "            t = t + 1 \n",
    "            current_year = current_year + 1\n",
    "                         \n",
    "            temp = torch.zeros(len(VA)).to(device).to(torch.float64)\n",
    "            \n",
    "            for author_year in tqdm(VA):\n",
    "                \n",
    "                # Block 1:\n",
    "                author_array = self.pnai(AS[t], VA, author_year, ta)\n",
    "                author_year_mapped = self.get_mapped_author_year(author_year)\n",
    "                pas[author_year_mapped] = (1. / (ta + 1)) * (torch.sum(author_array) + AS[t][author_year_mapped])\n",
    "                \n",
    "                sum_temp = torch.tensor([0.0]).to(device).to(torch.float64)\n",
    "                weighted_score_temp = torch.tensor([0.0]).to(device).to(torch.float64)\n",
    "                \n",
    "                # Block 2:\n",
    "                papers = self.get_all_papers_by_author_year(author_year_mapped).to(device)\n",
    "                for paper in papers:\n",
    "                    \n",
    "                    if current_year <= DATASET_LAST_YEAR:\n",
    "                        #Block 3: Weighted Citation by Author\n",
    "                        citation_count = self.find_papers_citing_papers_until_year(paper, current_year).to(device).to(torch.float64)\n",
    "                        weighted_score_temp = weighted_score_temp + self.W_pa(paper, author_year_mapped) * (citation_count.shape[0]) \n",
    "                                          \n",
    "                    sum_temp = sum_temp + (self.W_pa(paper, author_year_mapped) * PS[t][int(paper)])\n",
    "                \n",
    "                if current_year <= DATASET_LAST_YEAR:\n",
    "                    weighted_citation[t][author_year_mapped] = weighted_score_temp\n",
    "                \n",
    "                temp[author_year_mapped] = beta * sum_temp\n",
    "                \n",
    "                temp_compute = torch.tensor([0.0]).to(device).to(torch.float64)\n",
    "                aa = self.get_all_coauthors_and_citing_authors(author_year_mapped).to(device)\n",
    "                for author_found in aa:\n",
    "                    temp_compute = temp_compute + (self.W_aa(author_found, author_year_mapped) * temp[int(author_found)])\n",
    "                temp[author_year_mapped] = temp_compute + mu[int(year)] * temp[author_year_mapped]\n",
    "                tai = self.T_ai(author_year, sigma2, current_year)\n",
    "                AS[t+1][author_year_mapped] = lmbda * tai * temp[author_year_mapped] + (1. - lmbda) * torch.tensor([(1./len(VA))]).to(torch.float64).to(device)\n",
    "#                 print(\"TAI: \", lmbda * tai * temp[author_year_mapped] + (1. - lmbda) * torch.tensor([(1./len(VA))]).to(torch.float64).to(device), lmbda * tai * temp[author_year_mapped], (1. - lmbda), torch.tensor([(1./len(VA))]).to(torch.float64).to(device))\n",
    "                \n",
    "            checkpoint_path = os.path.join(self.checkpoint_path, str(t) + '_author_perf.pkl')\n",
    "            with open(checkpoint_path, 'wb') as f:\n",
    "                pkl.dump(pas, f)\n",
    "            \n",
    "            checkpoint_path = os.path.join(self.checkpoint_path, str(t) + '_author_score.pkl')\n",
    "            with open(checkpoint_path, 'wb') as f:\n",
    "                pkl.dump(AS[t+1], f)\n",
    "            \n",
    "            if current_year <= DATASET_LAST_YEAR:\n",
    "                checkpoint_path = os.path.join(self.checkpoint_path, str(t) + '_author_wcc_score.pkl')\n",
    "                with open(checkpoint_path, 'wb') as f:\n",
    "                    pkl.dump(weighted_citation[t], f)\n",
    "\n",
    "            temp = torch.zeros(len(VP)).to(device).to(torch.float64)   \n",
    "            for paper in tqdm(VP):\n",
    "                sum_authors = torch.tensor([0.0]).to(device).to(torch.float64)\n",
    "                paper_mapped = self.get_mapped_paper(paper)\n",
    "                paper_year = self.paper_published_year(paper_mapped)\n",
    "                if torch.numel(paper_year) != 0:\n",
    "                    authors_paper = self.get_all_authors_of_paper(paper_mapped).to(device)\n",
    "                    for author_year in authors_paper:\n",
    "                        sum_authors = sum_authors + (self.W_pa(paper_mapped, author_year) * pas[int(author_year)])\n",
    "                    temp[paper_mapped] = alpha * sum_authors\n",
    "                    papers_citing = self.find_papers_citing_paper(paper_mapped).to(device)\n",
    "                    temp_sum = torch.tensor([0.0]).to(device).to(torch.float64)\n",
    "                    for paper_citing in papers_citing:\n",
    "                        temp_sum = temp_sum + temp[int(paper_citing)]\n",
    "                    temp[paper_mapped] = temp_sum + mu[int(paper_year)] * temp[paper_mapped]\n",
    "#                         temp[paper_mapped] = temp[int(paper_citing)] + mu[int(paper_year)] * temp[paper_mapped]\n",
    "                    pi = self.T_pi(paper_mapped, current_year, sigma1)\n",
    "                    PS[t+1][paper_mapped] = lmbda * pi * temp[paper_mapped] + (1. - lmbda) * torch.tensor([(1./len(VP))]).to(torch.float64).to(device)\n",
    "\n",
    "            checkpoint_path = os.path.join(self.checkpoint_path, str(t) + '_paper_perf.pkl')\n",
    "            with open(checkpoint_path, 'wb') as f:\n",
    "                pkl.dump(PS[t+1], f)\n",
    "                \n",
    "            delta = torch.tensor([0.0]).to(device).to(torch.float64)\n",
    "            delta = delta + torch.sum(torch.abs(PS[t+1] - PS[t]))\n",
    "            delta = delta + torch.sum(torch.abs(AS[t+1] - AS[t]))\n",
    "            print(delta)\n",
    "#             print(PS[t+1])\n",
    "            \n",
    "        return AS, PS, weighted_citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmr = WMR_Rank(data, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['paper', 'written_by', 'author'].edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wmr.order('quant-ph/0001014', 'Rubin Morton H._2000', tomap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wmr.W_pp('astro-ph/0202187', 'hep-ph/0004227')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paper_info_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(authors_pd.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|█████████████████████████████████████████| 425/425 [00:05<00:00, 71.83it/s]\n",
      "100%|████████████████████████████████████████| 319/319 [00:00<00:00, 489.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.7078938363], device='cuda:0', dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|█████████████████████████████████████████| 425/425 [00:05<00:00, 72.16it/s]\n",
      "100%|████████████████████████████████████████| 319/319 [00:00<00:00, 491.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0136944653], device='cuda:0', dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|█████████████████████████████████████████| 425/425 [00:05<00:00, 72.05it/s]\n",
      "100%|████████████████████████████████████████| 319/319 [00:00<00:00, 494.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0003657380], device='cuda:0', dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|█████████████████████████████████████████| 425/425 [00:05<00:00, 71.66it/s]\n",
      "100%|████████████████████████████████████████| 319/319 [00:00<00:00, 491.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.4155248580e-05], device='cuda:0', dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "AS, PS, weighted_citation = wmr.algo(2001, papers_pd_list, authors_pd_list, torch.tensor([0.70]).to(torch.float64), torch.tensor([0.60]).to(torch.float64), torch.tensor([0.50]).to(torch.float64), torch.tensor([1.0]).to(torch.float64), torch.tensor([0.85]).to(torch.float64), torch.tensor([1e-6]).to(torch.float64), AUTHOR_PAST_YEARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0001707659, device='cuda:0', dtype=torch.float64) tensor(0.0001671356, device='cuda:0', dtype=torch.float64) tensor(0.0001670040, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(torch.std(PS[1]), torch.std(PS[2]), torch.std(PS[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT_PATH = \"./checkpoints/\"\n",
    "# author_path = os.path.join(CHECKPOINT_PATH, '0_author_wcc_score.pkl')\n",
    "# with open(author_path, 'rb') as f:\n",
    "#     AS = pkl.load(f)\n",
    "# print(AS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_correlation_coefficient(x, y):\n",
    "    vx = x - torch.mean(x)\n",
    "    vy = y - torch.mean(y)\n",
    "    return torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)))\n",
    "\n",
    "def spearman_correlation_coefficient(x, y):\n",
    "    x_rank = x.argsort()\n",
    "    y_rank = y.argsort()\n",
    "    return pearson_correlation_coefficient(x_rank, y_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pearson correlation coefficient and spearman correlation coefficient between AS and weighted citation by year\n",
    "pearson_correlation_coefficient_list = []\n",
    "spearman_correlation_coefficient_list = []\n",
    "for i in range(PREDICT_FOR_YEARS):\n",
    "    pearson_correlation_coefficient_list.append(pearson_correlation_coefficient(AS[i+1], weighted_citation[i]))\n",
    "    spearman_correlation_coefficient_list.append(spearman_correlation_coefficient(AS[i+1], weighted_citation[i]))\n",
    "\n",
    "\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Correlation Coefficient')\n",
    "plt.title('Correlation Coefficient between AS and weighted citation by year')\n",
    "plt.plot(pearson_correlation_coefficient_list, label='Pearson Correlation Coefficient')\n",
    "plt.plot(spearman_correlation_coefficient_list, label='Spearman Correlation Coefficient')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
