{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, listdir, mkdir\n",
    "from os.path import join\n",
    "import os\n",
    "from torch_geometric.data import HeteroData\n",
    "import json\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if gpu is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../unarXiv\"\n",
    "YEARS = ['00', '01']\n",
    "CATEGORIES = [\"cs.AI\", \"cs.CL\", \"cs.CV\", \"cs.LG\", \"stat.ML\"]\n",
    "AUTHOR_PAST_YEARS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# papers = read_jsonl(join(data_path, str(folder), files))\n",
    "# for paper in papers: \n",
    "#     metadata = paper.get('metadata', {})\n",
    "#     if metadata.get('authors_parsed'):\n",
    "#         print(metadata['authors_parsed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cited_paper_id, cited_paper_info in paper[\"bib_entries\"].items():\n",
    "#     if cited_paper_info.get('contained_arXiv_ids'):\n",
    "#         # print(len(cited_paper_info['contained_arXiv_ids']))\n",
    "#         cited_paper_ids.append(cited_paper_info['contained_arXiv_ids'][0].get('id')) # 1 Paper at a time \n",
    "#         # print(cited_paper_info['contained_arXiv_ids'][0].get('id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "total_papers = 0\n",
    "authors = []\n",
    "papers = []\n",
    "paper_info = pd.DataFrame(columns=['paper_id', 'title', 'authors', 'cited_paper_ids'])\n",
    "for folder in listdir(data_path):\n",
    "    if folder in YEARS:\n",
    "        print(folder, os.path.join(data_path, str(folder)))\n",
    "        if folder[0] == \"9\":\n",
    "            year = \"19\" + folder\n",
    "        else:\n",
    "            year = \"20\" + folder\n",
    "        for files in tqdm(listdir(join(data_path, str(folder)))):\n",
    "            papers_read = read_jsonl(join(data_path, str(folder), files))\n",
    "            total_papers += len(papers_read)\n",
    "            for paper in papers_read:\n",
    "                \n",
    "                cited_paper_ids = []\n",
    "                author_names = []\n",
    "                metadata = paper.get('metadata', {})\n",
    "                \n",
    "                if metadata.get('title') and metadata.get('authors_parsed') and metadata.get('categories'):\n",
    "                    \n",
    "                    paper_title = metadata['title']\n",
    "                    paper_id = paper[\"paper_id\"]\n",
    "                    categories = metadata['categories'].split('')\n",
    "                    found_match = any([cat in discipline for cat in categories])\n",
    "                    \n",
    "                    if found_match:\n",
    "                        \n",
    "                        papers.append(paper_id) # 1 Paper\n",
    "                        \n",
    "                        for cited_paper_id, cited_paper_info in paper[\"bib_entries\"].items():\n",
    "                            if 'ids' not in cited_paper_info:\n",
    "                                continue\n",
    "                            elif 'arxiv_id' not in cited_paper_info['ids']:\n",
    "                                continue\n",
    "                            else:\n",
    "                                ref_paper_id = cited_paper_info['ids']['arxiv_id']\n",
    "                                if ref_paper_id == '':\n",
    "                                    continue\n",
    "                                cited_paper_ids.append(ref_paper_id)\n",
    "                        papers.extend(cited_paper_ids)\n",
    "                        \n",
    "                        if metadata.get('authors_parsed'):\n",
    "                            \n",
    "                            authors_parsed = metadata['authors_parsed']\n",
    "                            \n",
    "                            # Consider only top 5 authors\n",
    "                            for author in authors_parsed[:5]:\n",
    "                                author = author[:2]\n",
    "                                author_parsed = \" \".join(author)\n",
    "                                author_names.append(author_parsed)\n",
    "                                \n",
    "                            author_names = list(set(author_names))\n",
    "                            authors.extend(author_names) # Multiple authors at a time\n",
    "                            \n",
    "                        paper_info = paper_info.append({'paper_id': paper_id, 'title': paper_title, 'authors': author_names, 'cited_paper_ids': cited_paper_ids, 'year': year}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_pd = pd.DataFrame(authors)\n",
    "authors_pd = authors_pd.drop_duplicates()\n",
    "papers_pd = pd.DataFrame(papers)\n",
    "papers_pd = papers_pd.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_info.to_csv('paper_info.csv', index=False)\n",
    "papers_pd.to_csv('papers.csv', index=False, header=False)\n",
    "authors_pd.to_csv('authors.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(authors_pd)\n",
    "print(papers_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,v in paper_info.iterrows():\n",
    "#     print(v['paper_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_paper_id = papers_pd[0].unique()\n",
    "unique_paper_id = pd.DataFrame(data={\n",
    "    'paperId': unique_paper_id,\n",
    "    'mappedID': pd.RangeIndex(len(unique_paper_id)),\n",
    "})\n",
    "print(\"Mapping of paper IDs to consecutive values:\")\n",
    "print(\"==========================================\")\n",
    "print(unique_paper_id.head())\n",
    "print()\n",
    "unique_paper_id.to_csv('unique_paper_id.csv', index=False, header=False)\n",
    "\n",
    "# for i,v in unique_paper_id.iterrows():\n",
    "#     print(v)\n",
    "\n",
    "# unique_author_id = authors_pd[0].unique()\n",
    "# unique_author_id = pd.DataFrame(data={\n",
    "#     'authorId': unique_author_id,\n",
    "#     'mappedID': pd.RangeIndex(len(unique_author_id)),\n",
    "# })\n",
    "# print(\"Mapping of author IDs to consecutive values, remember each author in a year is treated as a new entity:\")\n",
    "# print(\"==========================================\")\n",
    "# print(unique_author_id.head())\n",
    "# print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GENERATING THE HETEROGENOUS GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_info_2 = paper_info.explode('authors')\n",
    "paper_exploded = paper_info_2.reset_index(drop=True)\n",
    "paper_exploded['position'] = paper_exploded.groupby(['paper_id', 'title']).cumcount() + 1\n",
    "paper_info_2 = paper_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing unique total entities by treating authors in new years as seperate entities\n",
    "paper_info_2['authors_year'] = paper_info_2['authors'].astype(str) + '_' + paper_info_2['year'].astype(str)\n",
    "\n",
    "total_author_entities = len(paper_info_2['authors_year'].unique())\n",
    "unique_author_year_id = paper_info_2['authors_year'].unique()\n",
    "\n",
    "unique_author_year_id = pd.DataFrame(data={\n",
    "    'authorId': unique_author_year_id,\n",
    "    'mappedID': pd.RangeIndex(total_author_entities),\n",
    "})\n",
    "\n",
    "print(\"Total number of unique author entities: \", total_author_entities)\n",
    "\n",
    "# Using this to create the edges of the heterograph\n",
    "paper_info_3 = paper_info.explode('cited_paper_ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_pd_list = unique_author_year_id['authorId'].tolist()\n",
    "papers_pd_list = unique_paper_id['paperId'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(unique_paper_id)\n",
    "paper_id_info = pd.merge(paper_info_2['paper_id'], unique_paper_id, left_on='paper_id', right_on='paperId', how='left')\n",
    "# print(paper_id_info)\n",
    "paper_id_info = torch.from_numpy(paper_id_info['mappedID'].values)\n",
    "\n",
    "paper_author_year_info = pd.merge(paper_info_2['authors_year'], unique_author_year_id, left_on='authors_year', right_on='authorId', how='left')\n",
    "paper_author_year_info = torch.from_numpy(paper_author_year_info['mappedID'].values)\n",
    "\n",
    "paper_author_year_weight = torch.from_numpy(paper_info_2['position'].values)\n",
    "year_weight = torch.from_numpy(np.array(list(map(int,paper_info_2['year'].values))))\n",
    "\n",
    "attr_weights = torch.stack([paper_author_year_weight.float(), year_weight.float()], dim = 0)\n",
    "# for i in range(attr_weights[0].shape[0]):\n",
    "#     print(attr_weights[0][i])\n",
    "\n",
    "# Construct the paper-author edges:\n",
    "paper_author_edge_index = torch.stack([paper_id_info.float(), paper_author_year_info.float()], dim=0)\n",
    "\n",
    "paper_id_info = pd.merge(paper_info_3['paper_id'], unique_paper_id, left_on='paper_id', right_on='paperId', how='left')\n",
    "paper_id_info = torch.from_numpy(paper_id_info['mappedID'].values)\n",
    "\n",
    "paper_cite_info = pd.merge(paper_info_3['cited_paper_ids'], unique_paper_id, left_on='cited_paper_ids', right_on='paperId', how='left')\n",
    "paper_cite_info = torch.from_numpy(paper_cite_info['mappedID'].values)\n",
    "\n",
    "# Construct the paper-paper edges:\n",
    "paper_cite_edge_index = torch.stack([paper_id_info.float(), paper_cite_info.float()], dim=0)\n",
    "\n",
    "print()\n",
    "print(\"Edge indices:\")\n",
    "print(\"=============\")\n",
    "print(\"Final edge indices pointing from Paper to author:\")\n",
    "print(\"------------------------\")\n",
    "print(paper_author_edge_index)\n",
    "print()\n",
    "print(\"Final edge indices pointing from Paper to cited papers:\")\n",
    "print(\"------------------------\")\n",
    "print(paper_cite_edge_index)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import remove_self_loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = HeteroData()\n",
    "\n",
    "data[\"paper\"].node_id = torch.arange(len(unique_paper_id)).float()\n",
    "data[\"author\"].node_id = torch.arange(len(unique_author_year_id)).float()\n",
    "\n",
    "data[\"paper\", \"written_by\", \"author\"].edge_index = paper_author_edge_index\n",
    "data[\"paper\", \"cites\", \"paper\"].edge_index = paper_cite_edge_index\n",
    "data[\"paper\", \"written_by\", \"author\"].edge_attr = attr_weights\n",
    "\n",
    "print(data)\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WMR RANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"paper\", \"written_by\", \"author\"].edge_attr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MU = { 1991: 0.0, 1992: 0.0, 1993: 0.0, 1994: 0.0, 1995: 0.0, 1996: 0.0, 1997: 0.0, 1998: 0.0, 1999: 0.0,\n",
    "       2000: 0.1, 2001: 0.1, 2002: 0.1, 2003: 0.1, 2004: 0.1, 2005: 0.1, 2006: 0.2, 2007: 0.2, 2008: 0.2, \n",
    "       2009: 0.2, 2010: 0.3, 2011: 0.4, 2012: 0.5, 2013: 0.5, 2014: 0.5, 2015: 0.5, 2016: 0.6, 2017: 0.6, \n",
    "       2018: 0.7, 2019: 0.8, 2020: 0.8, 2021: 0.8, 2022: 0.8, 2023: 0.9 }\n",
    "\n",
    "class WMR_Rank:\n",
    "    \n",
    "    def __init__(self, heteroData, device):\n",
    "        self.checkpoint_path = './checkpoints/'\n",
    "        self.device = device\n",
    "        self.pp_edge_index = heteroData['paper', 'cites', 'paper'].edge_index.to(device)\n",
    "        self.pa_edge_index = heteroData['paper', 'written_by', 'author'].edge_index.to(device)\n",
    "        self.pa_edge_attr = heteroData['paper', 'written_by', 'author'].edge_attr.to(device)\n",
    "        \n",
    "        \n",
    "    def get_mapped_author_year(self, author_year):\n",
    "        return torch.tensor(unique_author_year_id[unique_author_year_id['authorId'] == author_year]['mappedID'].values[0]).to(self.device)\n",
    "    \n",
    "    def get_mapped_paper(self, paper):\n",
    "        return torch.tensor(unique_paper_id[unique_paper_id['paperId'] == paper]['mappedID'].values[0]).to(self.device)\n",
    "    \n",
    "    def get_unmapped_author_year(self, author_year):\n",
    "        return torch.tensor(unique_author_year_id[unique_author_year_id['mappedID'] == author_year]['authorId'].values[0]).to(self.device)\n",
    "    \n",
    "    def get_unmapped_paper(self, paper):\n",
    "        return torch.tensor(unique_paper_id[unique_paper_id['mappedID'] == paper]['paperId'].values[0]).to(self.device)\n",
    "    \n",
    "    def order(self, paper, author, tomap = False):\n",
    "        if tomap:\n",
    "            paper = self.get_mapped_paper(paper)\n",
    "            author = self.get_mapped_author_year(author)\n",
    "        result = self.pa_edge_attr[0][(self.pa_edge_index[0] == paper) & (self.pa_edge_index[1] == author)]\n",
    "        return result.to(device)\n",
    "        \n",
    "    def pnai(self, AS, VA, author_year, ta):\n",
    "        author = '_'.join(author_year.split('_')[:-1])\n",
    "        current_year = int(author_year.split('_')[-1])\n",
    "        author_perf = torch.zeros(ta).type(torch.float64).to(device)\n",
    "        for i in range(ta):\n",
    "            author_pass = author + '_' + str(current_year - i - 1)\n",
    "            if author_pass in VA:\n",
    "#                 print(\"PNAI: \", author_pass)\n",
    "                author_perf[i] = AS[self.get_mapped_author_year(author_pass)]\n",
    "        return author_perf.to(device)       \n",
    "    \n",
    "    def find_papers_citing_paper(self, paper):\n",
    "        papers_cited_mapped = torch.unique(self.pp_edge_index[0][self.pp_edge_index[1] == paper])\n",
    "        return papers_cited_mapped.to(device)\n",
    "    \n",
    "    def recent_papers_citing_paper(self, paper, publish_year, l):\n",
    "        device = self.device\n",
    "        citing_papers = self.find_papers_citing_paper(paper)\n",
    "        years_citing_papers = torch.Tensor().to(device)\n",
    "        for i in citing_papers:\n",
    "            years_citing_papers = torch.cat((years_citing_papers, self.paper_published_year(i)))\n",
    "        years_citing_papers, _ = torch.sort(years_citing_papers, descending = True)\n",
    "        \n",
    "        if (years_citing_papers.shape[0] < l) and (years_citing_papers.shape[0] != 0):\n",
    "            extra_years = torch.full((l - years_citing_papers.shape[0], ), publish_year)\n",
    "            years_citing_papers = torch.cat((years_citing_papers, extra_years))\n",
    "        years_citing_papers = years_citing_papers[:l]\n",
    "        return years_citing_papers.to(device)\n",
    "    \n",
    "    def T_pi(self, paper, current_year, sigma1):\n",
    "        publish_year = int(self.paper_published_year(paper))\n",
    "        evaluation_year = current_year\n",
    "        l = int(evaluation_year) - publish_year\n",
    "        years_citing_papers = self.recent_papers_citing_paper(paper, publish_year, l)\n",
    "        T_avg = torch.mean(years_citing_papers)\n",
    "        T_pi = torch.exp(-sigma1 * (current_year - T_avg))/ (current_year - T_avg)\n",
    "        return T_pi.to(device)\n",
    "    \n",
    "    def T_ai(self, author_year, sigma2, m = 10):\n",
    "        device = self.device\n",
    "        papers = self.get_all_papers_by_author_year(author_year)\n",
    "        citing_papers = torch.Tensor().to(device)\n",
    "        years_citing_papers = torch.Tensor().to(device)\n",
    "        citing_papers = self.pp_edge_index[0][torch.isin(self.pp_edge_index[1],papers)]\n",
    "        citing_papers = torch.unique(citing_papers)\n",
    "        for i in citing_papers:\n",
    "            years_citing_papers = torch.cat((years_citing_papers, self.paper_published_year(i)))\n",
    "        years_citing_papers, _ = torch.sort(years_citing_papers, descending = True)\n",
    "        year = int(author_year.split('_')[-1])\n",
    "        if years_citing_papers.shape[0] >= m:\n",
    "            years_citing_papers = years_citing_papers[:m]\n",
    "        elif years_citing_papers.shape[0] < m and (years_citing_papers.shape[0] != 0):\n",
    "            years_citing_papers = torch.cat((years_citing_papers, torch.full((m - years_citing_papers.shape[0], ), year)))\n",
    "        \n",
    "        T_avg = torch.mean(years_citing_papers)\n",
    "        T_ai = torch.exp(-sigma2 * (year - T_avg))/ (year - T_avg)\n",
    "        return T_ai.to(device)      \n",
    "        \n",
    "    def paper_paper_edge_exists(self, paper_i, paper_j):\n",
    "        check = torch.logical_and((self.pp_edge_index[0] == paper_j), (self.pp_edge_index[1] == paper_i))\n",
    "        has_edge = torch.any(check)\n",
    "        if has_edge:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "  \n",
    "    def get_all_papers_by_author_year(self, author_year):\n",
    "        papers = torch.unique(self.pa_edge_index[0][self.pa_edge_index[1] == author_year])\n",
    "        return papers.to(device)\n",
    "    \n",
    "          \n",
    "#     def paper_author_edge_exists(self, paper, author):\n",
    "#         paper_mapped = unique_paper_id[unique_paper_id['paperId'] == paper]['mappedID'].values[0]\n",
    "#         author_mapped = unique_author_id[unique_author_id['authorId'] == author]['mappedID'].values[0]\n",
    "#         edge_index = self.heteroData['paper', 'written_by', 'author'].edge_index\n",
    "#         has_edge = torch.where((edge_index[0] == paper_mapped) & (edge_index[1] == author_mapped))    \n",
    "#         if len(has_edge[0]) > 0:\n",
    "#             return True\n",
    "#         else:\n",
    "#             return False\n",
    "      \n",
    "        \n",
    "#     def get_papers_cited_by_paper(self, paper):\n",
    "#         paper_mapped = unique_paper_id[unique_paper_id['paperId'] == paper]['mappedID'].values[0]\n",
    "#         papers = self.pp_edge_index[1][self.pp_edge_index[0] == paper_mapped]\n",
    "#         papers = papers.numpy().tolist()\n",
    "#         if paper_mapped in papers:\n",
    "#             papers.remove(paper_mapped)\n",
    "#         papers = unique_paper_id[unique_paper_id['mappedID'].isin(papers)]['paperId'].values\n",
    "#         papers = list(set(papers))\n",
    "#         return papers\n",
    "    \n",
    "    def get_all_authors_of_paper(self, paper):\n",
    "        authors = torch.unique(self.pa_edge_index[1][self.pa_edge_index[0] == paper])\n",
    "        return authors.to(device)\n",
    "    \n",
    "    def get_all_coauthors_and_citing_authors(self, author_year):\n",
    "        device = self.device\n",
    "        papers_by_author = self.get_all_papers_by_author_year(author_year)\n",
    "        coauthors = torch.Tensor().to(device)\n",
    "        authors_that_cited = torch.Tensor().to(device)\n",
    "        for i in papers_by_author:\n",
    "            coauthors_paper = self.get_all_authors_of_paper(i)\n",
    "            citing_papers = self.find_papers_citing_paper(i)\n",
    "            coauthors = torch.cat((coauthors, coauthors_paper))\n",
    "            for j in citing_papers:\n",
    "                authors_that_cited = torch.cat((authors_that_cited, self.get_all_authors_of_paper(j)))\n",
    "        final_authors = torch.cat((authors_that_cited, coauthors))\n",
    "        final_authors = torch.unique(final_authors)\n",
    "        final_authors = final_authors[final_authors != author_year]\n",
    "        return final_authors.to(device)\n",
    "        \n",
    "        \n",
    "    def W_ca_raw(self, author_m, author_n, paper_i, paper_j):\n",
    "        return (1./ (self.order(paper_i, author_m) * self.order(paper_j, author_n))).to(device)\n",
    "\n",
    "    def W_ca(self, author_m, author_n, paper_i, paper_j):\n",
    "        device = self.device\n",
    "        num = self.W_ca_raw(author_m, author_n, paper_i, paper_j)\n",
    "        authors_k = self.get_all_authors_of_paper(paper_i)\n",
    "        authors_l = self.get_all_authors_of_paper(paper_j)\n",
    "        \n",
    "        denom = torch.zeros(1).to(device)\n",
    "        for k in range(authors_k.shape[0]):\n",
    "            for l in range(authors_l.shape[0]):\n",
    "                if authors_k[k] != authors_l[l]:\n",
    "                    denom = denom + self.W_ca_raw(authors_k[k], authors_l[l], paper_i, paper_j)\n",
    "        if denom != 0.0:\n",
    "            return (num/denom).to(device)\n",
    "        else:\n",
    "            return torch.zeros(1).to(device)\n",
    "\n",
    "    def W_ca_total(self, author_m, author_n):\n",
    "        device = self.device\n",
    "        p_i = self.get_all_papers_by_author_year(author_m)\n",
    "        p_j = self.get_all_papers_by_author_year(author_n)\n",
    "        sum_final = torch.zeros(1).to(device)\n",
    "        for i in range(p_i.shape[0]):\n",
    "            for j in range(p_j.shape[0]):\n",
    "                if self.paper_paper_edge_exists(p_i[i], p_j[j]) and (p_i[i] != p_j[j]):\n",
    "                    sum_final = sum_final + self.W_ca(author_m, author_n, p_i[i], p_j[j])\n",
    "        return sum_final.to(device)\n",
    "    \n",
    "    def W_coa_raw(self, author_i, author_j, paper):\n",
    "#         print(\"Check for Order: \", self.pa_edge_index[1][self.pa_edge_index[0] == paper])\n",
    "#         print(\"Author_i: \", author_i)\n",
    "#         print(\"Paper: \", paper)\n",
    "#         print(\"Order: \", self.order(paper, author_i))\n",
    "        return (1./ (self.order(paper, author_i) * self.order(paper, author_j))).to(device)\n",
    "    \n",
    "    def W_coa(self, author_i, author_j, p):\n",
    "        denom = torch.zeros(1, device = self.device)\n",
    "        authors = self.get_all_authors_of_paper(p)\n",
    "        if authors.shape[0] > 1:\n",
    "            for i in range(authors.shape[0]):\n",
    "                for j in range(authors.shape[0]):\n",
    "                    if authors[i] != authors[j]:\n",
    "                        denom = denom + self.W_coa_raw(authors[i], authors[j], p)\n",
    "            if denom != 0.0:\n",
    "                return self.W_coa_raw(author_i, author_j, p)/denom\n",
    "        else:\n",
    "            return torch.zeros(1).to(device)\n",
    "    \n",
    "    def W_coa_total(self, author_i, author_j):\n",
    "        papers_i = torch.unique(self.pa_edge_index[0][(self.pa_edge_index[1] == author_i)])\n",
    "        papers_j = torch.unique(self.pa_edge_index[0][(self.pa_edge_index[1] == author_j)])\n",
    "        papers_cat, counts = torch.cat([papers_i, papers_j]).unique(return_counts=True)\n",
    "        papers = papers_cat[torch.where(counts.gt(1))]\n",
    "        papers = torch.unique(papers)\n",
    "        sum_final = torch.zeros(1, device = self.device)\n",
    "        for paper in papers:\n",
    "            sum_final = sum_final + self.W_coa(author_i, author_j, paper)\n",
    "        return sum_final\n",
    "    \n",
    "    def W_aa(self, author_i, author_j):\n",
    "        result = self.W_ca_total(author_i, author_j)\n",
    "        result_2 = self.W_coa_total(author_i, author_j)\n",
    "        return result + result_2\n",
    "    \n",
    "    def W_pa(self, papers_array, author_year):\n",
    "        device = self.device\n",
    "        num_authors = torch.sum(self.pa_edge_index[0] == papers_array).to(device)\n",
    "        position_author = self.order(papers_array, author_year)\n",
    "        return ((2**(num_authors - position_author)) / ((2**num_authors) - 1)).to(self.device)\n",
    "    \n",
    "    def W_pp(self, p_i, p_j):\n",
    "        if self.paper_paper_edge_exists(p_i, p_j):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def paper_published_year(self, papers):\n",
    "        return torch.unique(self.pa_edge_attr[1][self.pa_edge_index[self.pa_edge_index == papers]]).to(device)\n",
    "    \n",
    "    def weighted_citation_by_author(self, author):\n",
    "        device = self.device\n",
    "        papers = self.get_all_papers_by_author_year(author).to(device)\n",
    "        score = torch.tensor([0.0]).to(device)\n",
    "        for paper in papers: \n",
    "            citation_count = self.find_papers_citing_paper(paper)\n",
    "            score = score + self.W_pa(paper, author) * (citation_count.shape[0])\n",
    "        return score\n",
    "            \n",
    "    # alpha = 0.70, beta = 0.60, lmbda = 0.85,  sigma_1 = 0.5, sigma_2 = 1.0, mu = [0.2, 0.8, 0.2], VA = list of authors_pd, VP = list of papers_pd\n",
    "    def algo(self, current_year, VP, VA, alpha, beta, sigma1, sigma2, lmbda, epsilon, ta=2):\n",
    "        # Assumption\n",
    "        mu = MU\n",
    "        device = self.device\n",
    "        \n",
    "        PS = torch.zeros(11, len(VP)).to(device) # Capped at 10 years\n",
    "        PS[0] = (torch.ones(len(VP)).to(device)) / len(VP)\n",
    "        \n",
    "        AS = torch.zeros(11, len(VA)).to(device)\n",
    "        AS[0] = (torch.ones(len(VA)).to(device)) / len(VA)\n",
    "        \n",
    "        pas = torch.ones(len(VA)).to(device)\n",
    "        # Represents the current_year - 1 in terms of symbolic notation,  1 represents first analysis year\n",
    "        t = -1         \n",
    "        delta = 2 * epsilon\n",
    "        while delta > epsilon and tqdm(t<11):\n",
    "            # First iteration 0\n",
    "            t = t + 1 \n",
    "            current_year = current_year + 1\n",
    "                         \n",
    "            sum_temp = 0\n",
    "            temp = torch.zeros(len(VA), device = device)\n",
    "            \n",
    "            for author_year in tqdm(VA):\n",
    "                \n",
    "                # Block 1:\n",
    "                author_array = self.pnai(AS[t], VA, author_year, ta)\n",
    "                author_year_mapped = self.get_mapped_author_year(author_year)\n",
    "                pas[author_year_mapped] = (1 / (ta + 1)) * (torch.sum(author_array) + AS[t][author_year_mapped])\n",
    "                \n",
    "                # Block 2:\n",
    "                author_year_mapped = self.get_mapped_author_year(author_year)\n",
    "                papers = self.get_all_papers_by_author_year(author_year_mapped).to(device)\n",
    "                for paper in papers:\n",
    "                    sum_temp = sum_temp + (self.W_pa(paper, author_year_mapped) * PS[t][int(paper)])\n",
    "                temp[author_year_mapped] = beta * sum_temp\n",
    "                \n",
    "                temp_compute = torch.tensor([0.0]).to(device)\n",
    "                aa = self.get_all_coauthors_and_citing_authors(author_year_mapped).to(device)\n",
    "                for author_found in aa:\n",
    "                    temp_compute = temp_compute + (self.W_aa(author_found, author_year_mapped) * temp[int(author_found)])\n",
    "                temp[author_year_mapped] = temp_compute + mu[int(year)] * temp[author_year_mapped]\n",
    "                AS[t+1][author_year_mapped] = lmbda * self.T_ai(author_year, current_year, sigma2) * temp[author_year_mapped] + (1 - lmbda) * (1/len(VA))\n",
    "            \n",
    "            checkpoint_path = os.path.join(self.checkpoint_path, str(t) + '_author_perf.pkl')\n",
    "            with open(checkpoint_path, 'wb') as f:\n",
    "                pkl.dump(pas, f)\n",
    "            \n",
    "            checkpoint_path = os.path.join(self.checkpoint_path, str(t) + '_author_score.pkl')\n",
    "            with open(checkpoint_path, 'wb') as f:\n",
    "                pkl.dump(AS[t+1], f)\n",
    "\n",
    "            temp = torch.zeros(len(VP))   \n",
    "            for paper in tqdm(VP):\n",
    "                sum_authors = torch.tensor([0.0]).to(device)\n",
    "                paper_mapped = self.get_mapped_paper(paper)\n",
    "                paper_year = int(self.paper_published_year(paper_mapped))\n",
    "                authors_paper = self.get_all_authors_of_paper(paper_mapped).to(device)\n",
    "                for author_year in authors_paper:\n",
    "                    sum_authors = sum_authors + (self.W_pa(paper_mapped, author_year) * pas[author_year])\n",
    "                temp[paper_mapped] = alpha * sum_authors\n",
    "                \n",
    "                papers_citing = self.find_papers_citing_paper(paper_mapped).to(device)\n",
    "                for paper_citing in papers_citing:\n",
    "                    temp[paper_mapped] = temp[int(paper_citing)] + mu[paper_year] * temp[paper_mapped]\n",
    "                PS[t+1][paper_mapped] = lmbda * self.T_pi(paper_mapped, current_year, sigma1) * temp[paper_mapped] + (1 - lmbda) * (1/len(VP))\n",
    "            \n",
    "            checkpoint_path = os.path.join(self.checkpoint_path, str(t) + '_paper_perf.pkl')\n",
    "            with open(checkpoint_path, 'wb') as f:\n",
    "                pkl.dump(PS[t+1], f)\n",
    "                \n",
    "            delta = 0\n",
    "            delta = delta + torch.sum(abs(PS[t+1] - PS[t]))\n",
    "            delta = delta + torch.sum(abs(AS[t+1] - AS[t]))\n",
    "    \n",
    "        return AS, PS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmr = WMR_Rank(data, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['paper', 'written_by', 'author'].edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmr.order('quant-ph/0001014', 'Rubin Morton H._2000', tomap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wmr.W_pp('astro-ph/0202187', 'hep-ph/0004227')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paper_info_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(authors_pd.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AS, PS = wmr.algo(2001, papers_pd_list, authors_pd_list, 0.70, 0.60, 0.5, 1.0, 0.85, 1e-6, AUTHOR_PAST_YEARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
