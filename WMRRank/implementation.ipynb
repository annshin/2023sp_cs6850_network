{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma952/.conda/envs/sets/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from os import path, listdir, mkdir\n",
    "from os.path import join\n",
    "import os\n",
    "from torch_geometric.data import HeteroData\n",
    "import json\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# check if gpu is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../unarXiv\"\n",
    "YEARS = ['00', '01']\n",
    "CATEGORIES = [\"cs.AI\", \"cs.CL\", \"cs.CV\", \"cs.LG\", \"stat.ML\"]\n",
    "AUTHOR_PAST_YEARS = 2\n",
    "PREDICT_FOR_YEARS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ma952/Sets/2023sp_cs6850_network/WMRRank'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# papers = read_jsonl(join(data_path, str(folder), files))\n",
    "# for paper in papers: \n",
    "#     metadata = paper.get('metadata', {})\n",
    "#     if metadata.get('authors_parsed'):\n",
    "#         print(metadata['authors_parsed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cited_paper_id, cited_paper_info in paper[\"bib_entries\"].items():\n",
    "#     if cited_paper_info.get('contained_arXiv_ids'):\n",
    "#         # print(len(cited_paper_info['contained_arXiv_ids']))\n",
    "#         cited_paper_ids.append(cited_paper_info['contained_arXiv_ids'][0].get('id')) # 1 Paper at a time \n",
    "#         # print(cited_paper_info['contained_arXiv_ids'][0].get('id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 ../unarXiv/00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 12/12 [00:55<00:00,  4.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01 ../unarXiv/01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 12/12 [00:59<00:00,  4.93s/it]\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "total_papers = 0\n",
    "authors = []\n",
    "papers = []\n",
    "paper_info = pd.DataFrame(columns=['paper_id', 'title', 'authors', 'cited_paper_ids'])\n",
    "for folder in listdir(data_path):\n",
    "    if folder in YEARS:\n",
    "        print(folder, os.path.join(data_path, str(folder)))\n",
    "        if folder[0] == \"9\":\n",
    "            year = \"19\" + folder\n",
    "        else:\n",
    "            year = \"20\" + folder\n",
    "        for files in tqdm(listdir(join(data_path, str(folder)))):\n",
    "            papers_read = read_jsonl(join(data_path, str(folder), files))\n",
    "            total_papers += len(papers_read)\n",
    "            for paper in papers_read:\n",
    "                \n",
    "                cited_paper_ids = []\n",
    "                author_names = []\n",
    "                metadata = paper.get('metadata', {})\n",
    "                \n",
    "                if metadata.get('title') and metadata.get('authors_parsed') and metadata.get('categories'):\n",
    "                    \n",
    "                    paper_title = metadata['title']\n",
    "                    paper_id = paper[\"paper_id\"]\n",
    "                    categories = metadata['categories'].split(' ')\n",
    "                    found_match = any([cat in CATEGORIES for cat in categories])\n",
    "                    \n",
    "                    if found_match:\n",
    "                        \n",
    "                        papers.append(paper_id) # 1 Paper\n",
    "                        \n",
    "                        for cited_paper_id, cited_paper_info in paper[\"bib_entries\"].items():\n",
    "                            if 'ids' not in cited_paper_info:\n",
    "                                continue\n",
    "                            elif 'arxiv_id' not in cited_paper_info['ids']:\n",
    "                                continue\n",
    "                            else:\n",
    "                                ref_paper_id = cited_paper_info['ids']['arxiv_id']\n",
    "                                if ref_paper_id == '':\n",
    "                                    continue\n",
    "                                cited_paper_ids.append(ref_paper_id)\n",
    "                        papers.extend(cited_paper_ids)\n",
    "                        \n",
    "                        if metadata.get('authors_parsed'):\n",
    "                            \n",
    "                            authors_parsed = metadata['authors_parsed']\n",
    "                            \n",
    "                            # Consider only top 5 authors\n",
    "                            for author in authors_parsed[:5]:\n",
    "                                author = author[:2]\n",
    "                                author_parsed = \" \".join(author)\n",
    "                                author_names.append(author_parsed)\n",
    "                                \n",
    "                            author_names = list(set(author_names))\n",
    "                            authors.extend(author_names) # Multiple authors at a time\n",
    "                            \n",
    "                        paper_info = paper_info.append({'paper_id': paper_id, 'title': paper_title, 'authors': author_names, 'cited_paper_ids': cited_paper_ids, 'year': year}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_pd = pd.DataFrame(authors)\n",
    "authors_pd = authors_pd.drop_duplicates()\n",
    "papers_pd = pd.DataFrame(papers)\n",
    "papers_pd = papers_pd.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_info.to_csv('paper_info.csv', index=False)\n",
    "papers_pd.to_csv('papers.csv', index=False, header=False)\n",
    "authors_pd.to_csv('authors.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         0\n",
      "0     Crutchfield James P.\n",
      "1    Shalizi Cosma Rohilla\n",
      "2          O'Rourke Joseph\n",
      "3             Schlei B. R.\n",
      "4                Prasad L.\n",
      "..                     ...\n",
      "595             Wets Geert\n",
      "596        Swinnen Gilbert\n",
      "597              Brijs Tom\n",
      "598    Bussche Jan Van den\n",
      "602          Geerts Floris\n",
      "\n",
      "[379 rows x 1 columns]\n",
      "              0\n",
      "0    cs/0001027\n",
      "1    cs/0001025\n",
      "2    cs/0001024\n",
      "3    cs/0001023\n",
      "4    cs/0001022\n",
      "..          ...\n",
      "353  cs/0112011\n",
      "354  cs/0112007\n",
      "355  cs/0112005\n",
      "357  cs/0112004\n",
      "358  cs/0112003\n",
      "\n",
      "[319 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(authors_pd)\n",
    "print(papers_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "612\n"
     ]
    }
   ],
   "source": [
    "print(len(authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,v in paper_info.iterrows():\n",
    "#     print(v['paper_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of paper IDs to consecutive values:\n",
      "==========================================\n",
      "      paperId  mappedID\n",
      "0  cs/0001027         0\n",
      "1  cs/0001025         1\n",
      "2  cs/0001024         2\n",
      "3  cs/0001023         3\n",
      "4  cs/0001022         4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_paper_id = papers_pd[0].unique()\n",
    "unique_paper_id = pd.DataFrame(data={\n",
    "    'paperId': unique_paper_id,\n",
    "    'mappedID': pd.RangeIndex(len(unique_paper_id)),\n",
    "})\n",
    "print(\"Mapping of paper IDs to consecutive values:\")\n",
    "print(\"==========================================\")\n",
    "print(unique_paper_id.head())\n",
    "print()\n",
    "unique_paper_id.to_csv('unique_paper_id.csv', index=False, header=False)\n",
    "\n",
    "# for i,v in unique_paper_id.iterrows():\n",
    "#     print(v)\n",
    "\n",
    "# unique_author_id = authors_pd[0].unique()\n",
    "# unique_author_id = pd.DataFrame(data={\n",
    "#     'authorId': unique_author_id,\n",
    "#     'mappedID': pd.RangeIndex(len(unique_author_id)),\n",
    "# })\n",
    "# print(\"Mapping of author IDs to consecutive values, remember each author in a year is treated as a new entity:\")\n",
    "# print(\"==========================================\")\n",
    "# print(unique_author_id.head())\n",
    "# print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GENERATING THE HETEROGENOUS GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_info_2 = paper_info.explode('authors')\n",
    "paper_exploded = paper_info_2.reset_index(drop=True)\n",
    "paper_exploded['position'] = paper_exploded.groupby(['paper_id', 'title']).cumcount() + 1\n",
    "paper_info_2 = paper_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique author entities:  425\n"
     ]
    }
   ],
   "source": [
    "# Computing unique total entities by treating authors in new years as seperate entities\n",
    "paper_info_2['authors_year'] = paper_info_2['authors'].astype(str) + '_' + paper_info_2['year'].astype(str)\n",
    "\n",
    "total_author_entities = len(paper_info_2['authors_year'].unique())\n",
    "unique_author_year_id = paper_info_2['authors_year'].unique()\n",
    "\n",
    "unique_author_year_id = pd.DataFrame(data={\n",
    "    'authorId': unique_author_year_id,\n",
    "    'mappedID': pd.RangeIndex(total_author_entities),\n",
    "})\n",
    "\n",
    "print(\"Total number of unique author entities: \", total_author_entities)\n",
    "\n",
    "# Using this to create the edges of the heterograph\n",
    "paper_info_3 = paper_info.explode('cited_paper_ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_pd_list = unique_author_year_id['authorId'].tolist()\n",
    "papers_pd_list = unique_paper_id['paperId'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Edge indices:\n",
      "=============\n",
      "Final edge indices pointing from Paper to author:\n",
      "------------------------\n",
      "tensor([[  0.,   0.,   1.,  ..., 318., 318., 318.],\n",
      "        [  0.,   1.,   2.,  ..., 287., 288., 285.]])\n",
      "\n",
      "Final edge indices pointing from Paper to cited papers:\n",
      "------------------------\n",
      "tensor([[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,  11.,\n",
      "          13.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,  22.,  23.,  24.,  25.,\n",
      "          26.,  27.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
      "          39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,  50.,\n",
      "          51.,  53.,  54.,  55.,  56.,  57.,  58.,  59.,  60.,  60.,  60.,  60.,\n",
      "          60.,  60.,  60.,  60.,  68.,  69.,  69.,  72.,  73.,  73.,  73.,  74.,\n",
      "          75.,  76.,  78.,  79.,  80.,  82.,  83.,  84.,  85.,  86.,  87.,  88.,\n",
      "          88.,  91.,  92.,  93.,  94.,  95.,  96.,  98.,  97.,  99., 100., 101.,\n",
      "         103., 104., 105., 106., 107., 108., 109., 110., 112., 113., 114., 115.,\n",
      "         117., 118., 119., 119., 122., 123., 124., 125., 126., 127., 127., 128.,\n",
      "         129., 131., 131., 131., 132., 133., 135., 136., 137., 138., 139., 140.,\n",
      "         141., 142., 143., 143., 146., 147., 148., 149., 150., 152., 153., 154.,\n",
      "         155., 156., 157., 159., 160., 161., 162., 164., 165., 166., 167., 168.,\n",
      "         169., 170., 171., 172., 173., 174., 175., 176., 178., 178., 178., 178.,\n",
      "         183., 184., 185., 186., 187., 188., 189., 190., 190., 190., 194., 195.,\n",
      "         196., 197., 198., 199., 200., 201., 202., 203., 204., 205., 206., 207.,\n",
      "         208., 209., 210., 211., 212., 212., 215., 216., 217., 218., 219., 220.,\n",
      "         221., 222., 223., 224., 225., 226., 227., 229., 230., 231., 232., 233.,\n",
      "         234., 235., 111., 236., 237., 238., 239., 240., 241., 242., 243., 244.,\n",
      "         245., 246., 247., 248., 249., 250., 251., 252., 253., 254., 255., 256.,\n",
      "         257., 258., 258., 260., 261., 262., 263., 264., 265., 266., 267., 268.,\n",
      "         269., 270., 270., 272., 273., 274., 275., 276., 278., 278., 280., 281.,\n",
      "         282., 283., 284., 285., 286., 287., 288., 289., 289., 291., 291., 292.,\n",
      "         293., 294., 295., 296., 297., 298., 299., 300., 301., 302., 303., 304.,\n",
      "         305., 305., 308., 310., 311., 312., 313., 314., 315., 316., 317., 318.],\n",
      "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  12.,\n",
      "          14.,  nan,  nan,  17.,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
      "          nan,  28.,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
      "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
      "          52.,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  61.,  62.,  63.,  64.,\n",
      "          65.,  65.,  66.,  67.,  nan,  70.,  71.,  nan,  64.,  65.,  66.,  nan,\n",
      "          nan,  77.,  nan,  nan,  81.,  nan,  nan,  nan,  nan,  nan,  nan,  89.,\n",
      "          90.,  nan,  nan,  nan,  nan,  nan,  97.,  nan,  nan,  97.,  97., 102.,\n",
      "          nan,  nan,  nan,  nan,  nan,  nan,  nan, 111.,  nan,  nan,  nan, 116.,\n",
      "          nan,  nan, 120., 121.,  nan, 123.,  nan,  nan,  nan,  64.,  65.,  nan,\n",
      "         130.,  70.,  64.,  65.,  nan, 134., 134.,  nan,  nan,  nan,  nan,  nan,\n",
      "          nan,  nan, 144., 145., 144.,  nan,  nan, 121., 151.,  nan,  nan,  nan,\n",
      "          nan, 144., 158., 121.,  nan,  nan, 163.,  nan,  nan,  nan,  nan,  nan,\n",
      "          nan,  nan,  nan,  nan, 121.,  nan,  nan, 177., 179., 180., 181., 182.,\n",
      "          nan, 134.,  nan,  nan,  nan,  nan,  nan, 191., 192., 193.,  nan,  nan,\n",
      "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
      "          nan,  nan,  nan,  nan, 213., 214.,  nan,  nan,  nan,  nan,  nan,  nan,\n",
      "          nan,  nan,  nan,  nan,  nan,  nan, 228.,  nan,  nan,  nan,  nan,  nan,\n",
      "          nan,  nan, 117.,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan, 163.,\n",
      "          nan,  nan,  nan, 130., 121., 121.,  nan,  nan,  nan,  nan, 121.,  nan,\n",
      "          nan, 259.,  97.,  nan,  nan,  nan,  nan, 265.,  nan,  nan,  nan,  nan,\n",
      "          nan, 271., 121.,  nan,  nan,  nan,  nan, 277., 279., 280., 279.,  nan,\n",
      "          nan,  nan,  nan,  nan, 130.,  nan,  nan, 228., 290.,  64.,  65.,  nan,\n",
      "          nan,  nan,  nan,  nan,  nan, 279.,  nan,  nan,  nan,  nan,  nan,  nan,\n",
      "         306., 307., 309., 121.,  nan,  nan,  nan,  nan,  nan, 225.,  nan,  nan]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(unique_paper_id)\n",
    "paper_id_info = pd.merge(paper_info_2['paper_id'], unique_paper_id, left_on='paper_id', right_on='paperId', how='left')\n",
    "# print(paper_id_info)\n",
    "paper_id_info = torch.from_numpy(paper_id_info['mappedID'].values)\n",
    "\n",
    "paper_author_year_info = pd.merge(paper_info_2['authors_year'], unique_author_year_id, left_on='authors_year', right_on='authorId', how='left')\n",
    "paper_author_year_info = torch.from_numpy(paper_author_year_info['mappedID'].values)\n",
    "\n",
    "paper_author_year_weight = torch.from_numpy(paper_info_2['position'].values)\n",
    "year_weight = torch.from_numpy(np.array(list(map(int,paper_info_2['year'].values))))\n",
    "\n",
    "attr_weights = torch.stack([paper_author_year_weight.float(), year_weight.float()], dim = 0)\n",
    "# for i in range(attr_weights[0].shape[0]):\n",
    "#     print(attr_weights[0][i])\n",
    "\n",
    "# Construct the paper-author edges:\n",
    "paper_author_edge_index = torch.stack([paper_id_info.float(), paper_author_year_info.float()], dim=0)\n",
    "\n",
    "paper_id_info = pd.merge(paper_info_3['paper_id'], unique_paper_id, left_on='paper_id', right_on='paperId', how='left')\n",
    "paper_id_info = torch.from_numpy(paper_id_info['mappedID'].values)\n",
    "\n",
    "paper_cite_info = pd.merge(paper_info_3['cited_paper_ids'], unique_paper_id, left_on='cited_paper_ids', right_on='paperId', how='left')\n",
    "paper_cite_info = torch.from_numpy(paper_cite_info['mappedID'].values)\n",
    "\n",
    "# Construct the paper-paper edges:\n",
    "paper_cite_edge_index = torch.stack([paper_id_info.float(), paper_cite_info.float()], dim=0)\n",
    "\n",
    "print()\n",
    "print(\"Edge indices:\")\n",
    "print(\"=============\")\n",
    "print(\"Final edge indices pointing from Paper to author:\")\n",
    "print(\"------------------------\")\n",
    "print(paper_author_edge_index)\n",
    "print()\n",
    "print(\"Final edge indices pointing from Paper to cited papers:\")\n",
    "print(\"------------------------\")\n",
    "print(paper_cite_edge_index)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import remove_self_loops\n",
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "print(torch_geometric.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001b[1mpaper\u001b[0m={ node_id=[319] },\n",
      "  \u001b[1mauthor\u001b[0m={ node_id=[425] },\n",
      "  \u001b[1m(paper, written_by, author)\u001b[0m={\n",
      "    edge_index=[2, 612],\n",
      "    edge_attr=[2, 612]\n",
      "  },\n",
      "  \u001b[1m(paper, cites, paper)\u001b[0m={ edge_index=[2, 300] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "data = HeteroData()\n",
    "\n",
    "data[\"paper\"].node_id = torch.arange(len(unique_paper_id)).float()\n",
    "data[\"author\"].node_id = torch.arange(len(unique_author_year_id)).float()\n",
    "\n",
    "data[\"paper\", \"written_by\", \"author\"].edge_index = paper_author_edge_index\n",
    "data[\"paper\", \"cites\", \"paper\"].edge_index = paper_cite_edge_index\n",
    "data[\"paper\", \"written_by\", \"author\"].edge_attr = attr_weights\n",
    "\n",
    "print(data)\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WMR RANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 1., 1., 2., 1., 2., 1., 2., 1., 2., 1., 1., 2., 1., 1., 2., 3.,\n",
       "        1., 2., 1., 1., 1., 2., 3., 1., 2., 3., 1., 2., 1., 2., 1., 2., 3., 1.,\n",
       "        1., 2., 1., 1., 2., 1., 1., 1., 2., 1., 2., 1., 2., 3., 1., 2., 3., 4.,\n",
       "        1., 2., 1., 2., 3., 1., 2., 1., 2., 1., 1., 1., 2., 1., 1., 2., 3., 1.,\n",
       "        2., 3., 1., 2., 3., 4., 1., 1., 2., 3., 1., 2., 1., 2., 1., 2., 3., 1.,\n",
       "        2., 3., 4., 1., 2., 3., 4., 5., 1., 1., 2., 3., 1., 2., 3., 1., 2., 1.,\n",
       "        2., 1., 2., 1., 2., 1., 2., 3., 1., 1., 2., 3., 1., 1., 1., 2., 3., 1.,\n",
       "        1., 1., 1., 1., 2., 3., 1., 2., 3., 4., 1., 2., 1., 1., 1., 2., 1., 2.,\n",
       "        3., 1., 1., 1., 2., 3., 1., 1., 1., 1., 1., 1., 2., 1., 2., 3., 1., 1.,\n",
       "        2., 3., 1., 2., 3., 1., 2., 1., 1., 2., 1., 2., 1., 2., 3., 1., 2., 3.,\n",
       "        1., 2., 1., 2., 3., 4., 1., 2., 1., 2., 3., 4., 1., 2., 3., 1., 1., 1.,\n",
       "        2., 3., 4., 5., 1., 2., 3., 4., 5., 1., 2., 3., 1., 1., 2., 1., 2., 1.,\n",
       "        2., 1., 2., 1., 2., 3., 1., 1., 2., 3., 4., 1., 2., 3., 1., 1., 2., 1.,\n",
       "        2., 3., 1., 2., 3., 4., 5., 1., 2., 1., 1., 2., 3., 1., 2., 3., 1., 2.,\n",
       "        3., 4., 1., 2., 1., 2., 3., 4., 1., 1., 2., 3., 4., 1., 2., 3., 1., 2.,\n",
       "        3., 4., 5., 1., 2., 3., 4., 1., 2., 3., 4., 1., 2., 1., 2., 3., 4., 5.,\n",
       "        1., 2., 1., 2., 1., 1., 2., 1., 2., 3., 4., 1., 2., 1., 2., 1., 2., 1.,\n",
       "        2., 3., 1., 1., 1., 2., 3., 4., 1., 2., 1., 1., 2., 1., 2., 3., 1., 1.,\n",
       "        2., 1., 2., 1., 2., 1., 1., 2., 1., 1., 2., 1., 2., 3., 1., 1., 2., 3.,\n",
       "        1., 2., 3., 1., 2., 1., 1., 2., 3., 4., 1., 1., 2., 1., 2., 3., 1., 2.,\n",
       "        3., 4., 5., 1., 2., 1., 2., 3., 1., 2., 3., 1., 1., 1., 2., 1., 1., 2.,\n",
       "        1., 2., 1., 2., 1., 2., 3., 1., 2., 1., 2., 1., 2., 3., 4., 1., 1., 1.,\n",
       "        1., 2., 1., 1., 2., 1., 1., 2., 3., 4., 5., 1., 2., 3., 4., 5., 1., 2.,\n",
       "        3., 4., 1., 2., 3., 4., 1., 2., 3., 1., 2., 3., 4., 5., 1., 2., 3., 1.,\n",
       "        2., 1., 1., 1., 1., 1., 1., 2., 3., 4., 1., 2., 3., 1., 2., 3., 4., 1.,\n",
       "        2., 1., 2., 1., 2., 3., 1., 2., 3., 4., 1., 1., 2., 1., 1., 2., 1., 1.,\n",
       "        1., 2., 3., 1., 2., 1., 2., 1., 2., 3., 4., 5., 1., 1., 1., 2., 1., 1.,\n",
       "        2., 1., 1., 2., 1., 2., 1., 1., 1., 1., 1., 2., 1., 2., 1., 2., 1., 2.,\n",
       "        3., 1., 2., 1., 2., 3., 4., 5., 1., 2., 3., 1., 1., 2., 3., 4., 5., 1.,\n",
       "        2., 3., 1., 2., 1., 2., 1., 1., 2., 3., 4., 5., 1., 1., 2., 3., 4., 1.,\n",
       "        1., 2., 1., 2., 3., 1., 1., 2., 1., 2., 3., 4., 1., 2., 3., 4., 1., 2.,\n",
       "        1., 1., 1., 2., 1., 1., 2., 3., 1., 2., 1., 2., 3., 1., 2., 3., 1., 2.,\n",
       "        3., 1., 1., 1., 2., 3., 1., 2., 3., 4., 5., 1., 2., 1., 2., 1., 1., 1.,\n",
       "        2., 3., 4., 5., 1., 2., 1., 2., 3., 1., 2., 1., 2., 3., 1., 2., 3., 4.],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"paper\", \"written_by\", \"author\"].edge_attr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "MU = { 1991: 0.0, 1992: 0.0, 1993: 0.0, 1994: 0.0, 1995: 0.0, 1996: 0.0, 1997: 0.0, 1998: 0.0, 1999: 0.0,\n",
    "       2000: 0.1, 2001: 0.1, 2002: 0.1, 2003: 0.1, 2004: 0.1, 2005: 0.1, 2006: 0.2, 2007: 0.2, 2008: 0.2, \n",
    "       2009: 0.2, 2010: 0.3, 2011: 0.4, 2012: 0.5, 2013: 0.5, 2014: 0.5, 2015: 0.5, 2016: 0.6, 2017: 0.6, \n",
    "       2018: 0.7, 2019: 0.8, 2020: 0.8, 2021: 0.8, 2022: 0.8, 2023: 0.9 }\n",
    "\n",
    "class WMR_Rank:\n",
    "    \n",
    "    def __init__(self, heteroData, device):\n",
    "        self.checkpoint_path = './checkpoints/'\n",
    "        self.device = device\n",
    "        self.pp_edge_index = heteroData['paper', 'cites', 'paper'].edge_index.to(device).to(torch.float64)\n",
    "        self.pa_edge_index = heteroData['paper', 'written_by', 'author'].edge_index.to(device).to(torch.float64)\n",
    "        self.pa_edge_attr = heteroData['paper', 'written_by', 'author'].edge_attr.to(device).to(torch.float64)\n",
    "        \n",
    "        \n",
    "    def get_mapped_author_year(self, author_year):\n",
    "        return torch.tensor(unique_author_year_id[unique_author_year_id['authorId'] == author_year]['mappedID'].values[0]).to(self.device)\n",
    "    \n",
    "    def get_mapped_paper(self, paper):\n",
    "        return torch.tensor(unique_paper_id[unique_paper_id['paperId'] == paper]['mappedID'].values[0]).to(self.device)\n",
    "    \n",
    "    def get_unmapped_author_year(self, author_year):\n",
    "        return torch.tensor(unique_author_year_id[unique_author_year_id['mappedID'] == author_year]['authorId'].values[0]).to(self.device)\n",
    "    \n",
    "    def get_unmapped_paper(self, paper):\n",
    "        return torch.tensor(unique_paper_id[unique_paper_id['mappedID'] == paper]['paperId'].values[0]).to(self.device)\n",
    "    \n",
    "    def order(self, paper, author, tomap = False):\n",
    "        if tomap:\n",
    "            paper = self.get_mapped_paper(paper)\n",
    "            author = self.get_mapped_author_year(author)\n",
    "        result = self.pa_edge_attr[0][(self.pa_edge_index[0] == paper) & (self.pa_edge_index[1] == author)]\n",
    "        return result.to(device)\n",
    "        \n",
    "    def pnai(self, AS, VA, author_year, ta):\n",
    "        author = '_'.join(author_year.split('_')[:-1])\n",
    "        current_year = int(author_year.split('_')[-1])\n",
    "        author_perf = torch.zeros(ta).type(torch.float64).to(device)\n",
    "        for i in range(ta):\n",
    "            author_pass = author + '_' + str(current_year - i - 1)\n",
    "            if author_pass in VA:\n",
    "#                 print(\"PNAI: \", author_pass)\n",
    "                author_perf[i] = AS[self.get_mapped_author_year(author_pass)]\n",
    "        return author_perf.to(device)       \n",
    "    \n",
    "    def find_papers_citing_paper(self, paper):\n",
    "        papers_cited_mapped = torch.unique(self.pp_edge_index[0][self.pp_edge_index[1] == paper])\n",
    "        return papers_cited_mapped.to(device)\n",
    "    \n",
    "    def recent_papers_citing_paper(self, paper, publish_year, l):\n",
    "        device = self.device\n",
    "        citing_papers = self.find_papers_citing_paper(paper)\n",
    "        years_citing_papers = torch.Tensor().to(device).to(torch.float64)\n",
    "        for i in citing_papers:\n",
    "            years_citing_papers = torch.cat((years_citing_papers, self.paper_published_year(i)))\n",
    "        years_citing_papers, _ = torch.sort(years_citing_papers, descending = True)\n",
    "        \n",
    "        if (years_citing_papers.shape[0] < l):\n",
    "            extra_years = torch.full((l - years_citing_papers.shape[0], ), publish_year).to(device)\n",
    "            years_citing_papers = torch.cat((years_citing_papers, extra_years))\n",
    "        years_citing_papers = years_citing_papers[:l]\n",
    "        return years_citing_papers.to(device)\n",
    "    \n",
    "    def T_pi(self, paper, current_year, sigma1):\n",
    "        publish_year = int(self.paper_published_year(paper))\n",
    "        evaluation_year = current_year\n",
    "        l = int(evaluation_year) - publish_year\n",
    "        years_citing_papers = self.recent_papers_citing_paper(paper, publish_year, l)\n",
    "        years_citing_papers = years_citing_papers.to(torch.float64)\n",
    "        T_avg = torch.mean(years_citing_papers).to(torch.float64)\n",
    "        T_pi = torch.exp(-sigma1 * (current_year - T_avg))/ (current_year - T_avg)\n",
    "        return T_pi.to(device).to(torch.float64)\n",
    "    \n",
    "    def T_ai(self, author_year, sigma2, current_year, m = 10):\n",
    "        device = self.device\n",
    "        papers = self.get_all_papers_by_author_year(author_year)\n",
    "        citing_papers = torch.Tensor().to(device).to(torch.float64)\n",
    "        years_citing_papers = torch.Tensor().to(device).to(torch.float64)\n",
    "        citing_papers = self.pp_edge_index[0][torch.isin(self.pp_edge_index[1], papers)]\n",
    "        citing_papers = torch.unique(citing_papers)\n",
    "        for i in citing_papers:\n",
    "            years_citing_papers = torch.cat((years_citing_papers, self.paper_published_year(i)))\n",
    "        years_citing_papers, _ = torch.sort(years_citing_papers, descending = True)\n",
    "        year = float(author_year.split('_')[-1])\n",
    "        if years_citing_papers.shape[0] >= m:\n",
    "            years_citing_papers = years_citing_papers[:m]\n",
    "        elif (years_citing_papers.shape[0] < m):\n",
    "            extra_years = torch.full((m - years_citing_papers.shape[0], ), year).to(device).to(torch.float64)\n",
    "            years_citing_papers = torch.cat((years_citing_papers, extra_years))\n",
    "            \n",
    "        T_avg = torch.mean(years_citing_papers).to(device).to(torch.float64)\n",
    "        T_ai = torch.exp(-sigma2 * (current_year - T_avg))/ (current_year - T_avg)\n",
    "        return T_ai.to(device).to(torch.float64)    \n",
    "        \n",
    "    def paper_paper_edge_exists(self, paper_i, paper_j):\n",
    "        check = torch.logical_and((self.pp_edge_index[0] == paper_j), (self.pp_edge_index[1] == paper_i))\n",
    "        has_edge = torch.any(check)\n",
    "        if has_edge:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "  \n",
    "    def get_all_papers_by_author_year(self, author_year):\n",
    "        papers = torch.unique(self.pa_edge_index[0][self.pa_edge_index[1] == author_year])\n",
    "        return papers.to(device)\n",
    "    \n",
    "          \n",
    "#     def paper_author_edge_exists(self, paper, author):\n",
    "#         paper_mapped = unique_paper_id[unique_paper_id['paperId'] == paper]['mappedID'].values[0]\n",
    "#         author_mapped = unique_author_id[unique_author_id['authorId'] == author]['mappedID'].values[0]\n",
    "#         edge_index = self.heteroData['paper', 'written_by', 'author'].edge_index\n",
    "#         has_edge = torch.where((edge_index[0] == paper_mapped) & (edge_index[1] == author_mapped))    \n",
    "#         if len(has_edge[0]) > 0:\n",
    "#             return True\n",
    "#         else:\n",
    "#             return False\n",
    "      \n",
    "        \n",
    "#     def get_papers_cited_by_paper(self, paper):\n",
    "#         paper_mapped = unique_paper_id[unique_paper_id['paperId'] == paper]['mappedID'].values[0]\n",
    "#         papers = self.pp_edge_index[1][self.pp_edge_index[0] == paper_mapped]\n",
    "#         papers = papers.numpy().tolist()\n",
    "#         if paper_mapped in papers:\n",
    "#             papers.remove(paper_mapped)\n",
    "#         papers = unique_paper_id[unique_paper_id['mappedID'].isin(papers)]['paperId'].values\n",
    "#         papers = list(set(papers))\n",
    "#         return papers\n",
    "    \n",
    "    def get_all_authors_of_paper(self, paper):\n",
    "        authors = torch.unique(self.pa_edge_index[1][self.pa_edge_index[0] == paper])\n",
    "        return authors.to(device)\n",
    "    \n",
    "    def get_all_coauthors_and_citing_authors(self, author_year):\n",
    "        device = self.device\n",
    "        papers_by_author = self.get_all_papers_by_author_year(author_year)\n",
    "        coauthors = torch.Tensor().to(device).to(torch.float64)\n",
    "        authors_that_cited = torch.Tensor().to(device).to(torch.float64)\n",
    "        for i in papers_by_author:\n",
    "            coauthors_paper = self.get_all_authors_of_paper(i)\n",
    "            citing_papers = self.find_papers_citing_paper(i)\n",
    "            coauthors = torch.cat((coauthors, coauthors_paper))\n",
    "            for j in citing_papers:\n",
    "                authors_that_cited = torch.cat((authors_that_cited, self.get_all_authors_of_paper(j)))\n",
    "        final_authors = torch.cat((authors_that_cited, coauthors))\n",
    "        final_authors = torch.unique(final_authors)\n",
    "        final_authors = final_authors[final_authors != author_year]\n",
    "        return final_authors.to(device).to(torch.float64)\n",
    "        \n",
    "        \n",
    "    def W_ca_raw(self, author_m, author_n, paper_i, paper_j):\n",
    "        return (1./ (self.order(paper_i, author_m) * self.order(paper_j, author_n))).to(device).to(torch.float64)\n",
    "\n",
    "    def W_ca(self, author_m, author_n, paper_i, paper_j):\n",
    "        device = self.device\n",
    "        num = self.W_ca_raw(author_m, author_n, paper_i, paper_j)\n",
    "        authors_k = self.get_all_authors_of_paper(paper_i)\n",
    "        authors_l = self.get_all_authors_of_paper(paper_j)\n",
    "        \n",
    "        denom = torch.zeros(1).to(device).to(torch.float64)\n",
    "        for k in range(authors_k.shape[0]):\n",
    "            for l in range(authors_l.shape[0]):\n",
    "                if authors_k[k] != authors_l[l]:\n",
    "                    denom = denom + self.W_ca_raw(authors_k[k], authors_l[l], paper_i, paper_j)\n",
    "        if denom != 0.0:\n",
    "            return (num/denom).to(device)\n",
    "        else:\n",
    "            return torch.zeros(1).to(device).to(torch.float64)\n",
    "\n",
    "    def W_ca_total(self, author_m, author_n):\n",
    "        device = self.device\n",
    "        p_i = self.get_all_papers_by_author_year(author_m)\n",
    "        p_j = self.get_all_papers_by_author_year(author_n)\n",
    "        sum_final = torch.zeros(1).to(device).to(torch.float64)\n",
    "        for i in range(p_i.shape[0]):\n",
    "            for j in range(p_j.shape[0]):\n",
    "                if self.paper_paper_edge_exists(p_i[i], p_j[j]) and (p_i[i] != p_j[j]):\n",
    "                    sum_final = sum_final + self.W_ca(author_m, author_n, p_i[i], p_j[j])\n",
    "        return sum_final.to(device)\n",
    "    \n",
    "    def W_coa_raw(self, author_i, author_j, paper):\n",
    "#         print(\"Check for Order: \", self.pa_edge_index[1][self.pa_edge_index[0] == paper])\n",
    "#         print(\"Author_i: \", author_i)\n",
    "#         print(\"Paper: \", paper)\n",
    "#         print(\"Order: \", self.order(paper, author_i))\n",
    "        return (1./ (self.order(paper, author_i) * self.order(paper, author_j))).to(device).to(torch.float64)\n",
    "    \n",
    "    def W_coa(self, author_i, author_j, p):\n",
    "        denom = torch.zeros(1, device = self.device).to(torch.float64)\n",
    "        authors = self.get_all_authors_of_paper(p)\n",
    "        if authors.shape[0] > 1:\n",
    "            for i in range(authors.shape[0]):\n",
    "                for j in range(authors.shape[0]):\n",
    "                    if authors[i] != authors[j]:\n",
    "                        denom = denom + self.W_coa_raw(authors[i], authors[j], p)\n",
    "            if denom != 0.0:\n",
    "                return self.W_coa_raw(author_i, author_j, p)/denom\n",
    "        else:\n",
    "            return torch.zeros(1).to(device).to(torch.float64)\n",
    "    \n",
    "    def W_coa_total(self, author_i, author_j):\n",
    "        papers_i = torch.unique(self.pa_edge_index[0][(self.pa_edge_index[1] == author_i)])\n",
    "        papers_j = torch.unique(self.pa_edge_index[0][(self.pa_edge_index[1] == author_j)])\n",
    "        papers_cat, counts = torch.cat([papers_i, papers_j]).unique(return_counts=True)\n",
    "        papers = papers_cat[torch.where(counts.gt(1))]\n",
    "        papers = torch.unique(papers)\n",
    "        sum_final = torch.zeros(1, device = self.device).to(torch.float64)\n",
    "        for paper in papers:\n",
    "            sum_final = sum_final + self.W_coa(author_i, author_j, paper)\n",
    "        return sum_final\n",
    "    \n",
    "    def W_aa(self, author_i, author_j):\n",
    "        result = self.W_ca_total(author_i, author_j)\n",
    "        result_2 = self.W_coa_total(author_i, author_j)\n",
    "        return result + result_2\n",
    "    \n",
    "    def W_pa(self, papers_array, author_year):\n",
    "        device = self.device\n",
    "        num_authors = torch.sum(self.pa_edge_index[0] == papers_array).to(device).to(torch.float64)\n",
    "        position_author = self.order(papers_array, author_year)\n",
    "        return ((2.**(num_authors - position_author)) / ((2.**num_authors) - 1.0)).to(self.device)\n",
    "    \n",
    "    def W_pp(self, p_i, p_j):\n",
    "        if self.paper_paper_edge_exists(p_i, p_j):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def paper_published_year(self, papers):\n",
    "        return torch.unique(self.pa_edge_attr[1][self.pa_edge_index[0] == papers]).to(device)\n",
    "    \n",
    "    def weighted_citation_by_author(self, author):\n",
    "        device = self.device\n",
    "        papers = self.get_all_papers_by_author_year(author).to(device)\n",
    "        score = torch.tensor([0.0]).to(device).to(torch.float64)\n",
    "        for paper in papers: \n",
    "            citation_count = self.find_papers_citing_paper(paper)\n",
    "            score = score + self.W_pa(paper, author) * (citation_count.shape[0])\n",
    "        return score\n",
    "            \n",
    "    # alpha = 0.70, beta = 0.60, lmbda = 0.85,  sigma_1 = 0.5, sigma_2 = 1.0, mu = [0.2, 0.8, 0.2], VA = list of authors_pd, VP = list of papers_pd\n",
    "    def algo(self, current_year, VP, VA, alpha, beta, sigma1, sigma2, lmbda, epsilon, ta=2):\n",
    "        # Assumption\n",
    "        mu = MU\n",
    "        device = self.device\n",
    "        \n",
    "        PS = torch.zeros(PREDICT_FOR_YEARS, len(VP)).to(device) # Capped at 10 years\n",
    "        PS = PS.to(torch.float64)\n",
    "        PS[0] = (torch.ones(len(VP)) / len(VP)).to(torch.float64)\n",
    "        \n",
    "        AS = torch.zeros(PREDICT_FOR_YEARS, len(VA)).to(device)\n",
    "        AS = AS.to(torch.float64)\n",
    "        AS[0] = ((torch.ones(len(VA)) / len(VA))).to(torch.float64)\n",
    "        \n",
    "        pas = torch.ones(len(VA)).to(device).to(torch.float64)\n",
    "        # Represents the current_year - 1 in terms of symbolic notation,  1 represents first analysis year\n",
    "        t = -1         \n",
    "        delta = torch.tensor([2 * epsilon]).to(torch.float64)\n",
    "        while delta > epsilon and tqdm(t < (PREDICT_FOR_YEARS - 1)):\n",
    "            # First iteration 0\n",
    "            t = t + 1 \n",
    "            current_year = current_year + 1\n",
    "                         \n",
    "            temp = torch.zeros(len(VA)).to(device).to(torch.float64)\n",
    "            \n",
    "            for author_year in tqdm(VA):\n",
    "                \n",
    "                # Block 1:\n",
    "                author_array = self.pnai(AS[t], VA, author_year, ta)\n",
    "                author_year_mapped = self.get_mapped_author_year(author_year)\n",
    "                pas[author_year_mapped] = (1 / (ta + 1)) * (torch.sum(author_array) + AS[t][author_year_mapped])\n",
    "                \n",
    "                sum_temp = torch.tensor([0.0]).to(device).to(torch.float64)\n",
    "                \n",
    "                # Block 2:\n",
    "                author_year_mapped = self.get_mapped_author_year(author_year)\n",
    "                papers = self.get_all_papers_by_author_year(author_year_mapped).to(device)\n",
    "                for paper in papers:\n",
    "                    sum_temp = sum_temp + (self.W_pa(paper, author_year_mapped) * PS[t][int(paper)])\n",
    "                temp[author_year_mapped] = beta * sum_temp\n",
    "                \n",
    "                temp_compute = torch.tensor([0.0]).to(device).to(torch.float64)\n",
    "                aa = self.get_all_coauthors_and_citing_authors(author_year_mapped).to(device)\n",
    "                for author_found in aa:\n",
    "                    temp_compute = temp_compute + (self.W_aa(author_found, author_year_mapped) * temp[int(author_found)])\n",
    "                temp[author_year_mapped] = temp_compute + mu[int(year)] * temp[author_year_mapped]\n",
    "                tai = self.T_ai(author_year, sigma2, current_year)\n",
    "                AS[t+1][author_year_mapped] = lmbda * tai * temp[author_year_mapped] + (1. - lmbda) * (1./len(VA))\n",
    "            \n",
    "            checkpoint_path = os.path.join(self.checkpoint_path, str(t) + '_author_perf.pkl')\n",
    "            with open(checkpoint_path, 'wb') as f:\n",
    "                pkl.dump(pas, f)\n",
    "            \n",
    "            checkpoint_path = os.path.join(self.checkpoint_path, str(t) + '_author_score.pkl')\n",
    "            with open(checkpoint_path, 'wb') as f:\n",
    "                pkl.dump(AS[t+1], f)\n",
    "\n",
    "            temp = torch.zeros(len(VP)).to(device).to(torch.float64)   \n",
    "            for paper in tqdm(VP):\n",
    "                sum_authors = torch.tensor([0.0]).to(device)\n",
    "                paper_mapped = self.get_mapped_paper(paper)\n",
    "                paper_year = self.paper_published_year(paper_mapped)\n",
    "                if torch.numel(paper_year) != 0:\n",
    "                    authors_paper = self.get_all_authors_of_paper(paper_mapped).to(device)\n",
    "                    for author_year in authors_paper:\n",
    "                        sum_authors = sum_authors + (self.W_pa(paper_mapped, author_year) * pas[int(author_year)])\n",
    "                    temp[paper_mapped] = alpha * sum_authors\n",
    "                    papers_citing = self.find_papers_citing_paper(paper_mapped).to(device)\n",
    "                    for paper_citing in papers_citing:\n",
    "                        temp[paper_mapped] = temp[int(paper_citing)] + mu[int(paper_year)] * temp[paper_mapped]\n",
    "                    pi = self.T_pi(paper_mapped, current_year, sigma1)\n",
    "                    PS[t+1][paper_mapped] = lmbda * pi * temp[paper_mapped] + (1. - lmbda) * (1./len(VP))\n",
    "\n",
    "            checkpoint_path = os.path.join(self.checkpoint_path, str(t) + '_paper_perf.pkl')\n",
    "            with open(checkpoint_path, 'wb') as f:\n",
    "                pkl.dump(PS[t+1], f)\n",
    "                \n",
    "            delta = torch.tensor([0.0]).to(device).to(torch.float64)\n",
    "            delta = delta + torch.sum(torch.abs(PS[t+1] - PS[t]))\n",
    "            delta = delta + torch.sum(torch.abs(AS[t+1] - AS[t]))\n",
    "            print(delta)\n",
    "    \n",
    "        return AS, PS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmr = WMR_Rank(data, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 2.0000e+00, 1.0000e+00,  ..., 2.0000e+00, 3.0000e+00,\n",
       "         4.0000e+00],\n",
       "        [2.0000e+03, 2.0000e+03, 2.0000e+03,  ..., 2.0010e+03, 2.0010e+03,\n",
       "         2.0010e+03]], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['paper', 'written_by', 'author'].edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wmr.order('quant-ph/0001014', 'Rubin Morton H._2000', tomap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wmr.W_pp('astro-ph/0202187', 'hep-ph/0004227')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       paper_id                                              title  \\\n",
      "0    cs/0001027      Pattern Discovery and Computational Mechanics   \n",
      "1    cs/0001027      Pattern Discovery and Computational Mechanics   \n",
      "2    cs/0001025                   Computational Geometry Column 38   \n",
      "3    cs/0001024  A Parallel Algorithm for Dilated Contour Extra...   \n",
      "4    cs/0001024  A Parallel Algorithm for Dilated Contour Extra...   \n",
      "..          ...                                                ...   \n",
      "607  cs/0112004  Part of Speech Tagging in Thai Language Using ...   \n",
      "608  cs/0112003  Using a Support-Vector Machine for Japanese-to...   \n",
      "609  cs/0112003  Using a Support-Vector Machine for Japanese-to...   \n",
      "610  cs/0112003  Using a Support-Vector Machine for Japanese-to...   \n",
      "611  cs/0112003  Using a Support-Vector Machine for Japanese-to...   \n",
      "\n",
      "                   authors cited_paper_ids  year  position  \\\n",
      "0     Crutchfield James P.              []  2000         1   \n",
      "1    Shalizi Cosma Rohilla              []  2000         2   \n",
      "2          O'Rourke Joseph              []  2000         1   \n",
      "3             Schlei B. R.              []  2000         1   \n",
      "4                Prasad L.              []  2000         2   \n",
      "..                     ...             ...   ...       ...   \n",
      "607        Isahara Hitoshi              []  2001         3   \n",
      "608                Ma Qing              []  2001         1   \n",
      "609          Murata Masaki              []  2001         2   \n",
      "610      Uchimoto Kiyotaka              []  2001         3   \n",
      "611        Isahara Hitoshi              []  2001         4   \n",
      "\n",
      "                   authors_year  \n",
      "0     Crutchfield James P._2000  \n",
      "1    Shalizi Cosma Rohilla_2000  \n",
      "2          O'Rourke Joseph_2000  \n",
      "3             Schlei B. R._2000  \n",
      "4                Prasad L._2000  \n",
      "..                          ...  \n",
      "607        Isahara Hitoshi_2001  \n",
      "608                Ma Qing_2001  \n",
      "609          Murata Masaki_2001  \n",
      "610      Uchimoto Kiyotaka_2001  \n",
      "611        Isahara Hitoshi_2001  \n",
      "\n",
      "[612 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(paper_info_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Crutchfield James P.'], ['Shalizi Cosma Rohilla'], [\"O'Rourke Joseph\"], ['Schlei B. R.'], ['Prasad L.'], ['Jelinek Frederick'], ['Chelba Ciprian'], ['Halpern Joseph Y.'], ['Lakemeyer Gerhard'], ['Lee Lillian'], ['Hess M.'], ['Berri J.'], ['Molla D.'], ['Vidal Jose M.'], ['Durfee Edmund H.'], ['Akuzawa Toshinao'], ['Zadrozny Wlodek'], ['Yuan Li-Yan'], ['Shen Yi-Dong'], ['You Jia-Huai'], ['Nicolas P.'], ['Saubion F.'], ['Stephan I.'], ['Ghrist Robert'], ['Koditschek Daniel'], ['Truszczynski Miroslaw'], ['East Deborah'], ['Denecker Marc'], ['Marek Victor W.'], ['Renals Steve'], ['Gotoh Yoshihiko'], ['Choi Freddy Y. Y.'], ['Apt Krzysztof R.'], ['Siebert Andreas'], ['Monfroy Eric'], ['Tung Anthony K. H.'], ['Lee Jason W. H.'], ['Tay Y. C.'], ['Vandecasteele Henk'], ['de Waal D. Andre'], ['Bruynooghe Maurice'], ['Arens Roman G.'], ['Busemann Stephan'], ['Schmeier Sven'], ['Sims Aidan'], ['Williams Mary-Anne'], ['Warren D.'], ['Castro L.'], ['Brants Thorsten'], ['Schaub Torsten'], ['Delgrande James'], ['Wassermann Renata'], ['Toni Francesca'], ['Miller Rob'], ['Kakas Antonis'], ['Otero Ramon P.'], ['Cabarcos Manuel'], ['Cabalar Pedro'], ['Zhou Neng-Fa'], ['Darwiche Adnan'], ['Erdem Esra'], ['Lifschitz Vladimir'], ['Babovich Yuliya'], ['Bovens Luc'], ['Hartmann Stephan'], ['Shapiro Stuart C.'], ['Johnson Frances L.'], ['Niemela Ilkka'], ['Hietalahti Maarit'], ['Massacci Fabio'], ['Eiter Thomas'], ['Egly Uwe'], ['Woltran Stefan'], ['Tompits Hans'], ['Koch Christoph'], ['Pfeifer Gerald'], ['Faber Wolfgang'], ['Leone Nicola'], ['Brewka Gerhard'], ['Simons Patrik'], ['Syrjanen Tommi'], ['Grosskreutz Henrik'], ['Vodislav Carmen'], ['Mercer Robert E.'], ['Van Nuffelen Bert'], ['Bouchon-Meunier Bernadette'], ['Mellouli Nedra'], ['Delgrande James P.'], ['Lukasiewicz Thomas'], ['Arlo-Costa Horacio'], ['Chopra Samir'], ['Georgatos Konstantinos'], ['Parikh Rohit'], ['Booth Richard'], [\"Dupre' Daniele Theseider\"], ['Meyer Thomas'], ['Lau Raymond'], ['ter Hofstede Arthur H. M.'], ['Bruza Peter D.'], ['Billigton D.'], ['Governatori G.'], ['Maher M. J.'], ['Antoniou G.'], ['Kern-Isberner Gabriele'], ['Satoh Ken'], ['Okamoto Hidenori'], ['Bialek William'], ['Tishby Naftali'], ['Pereira Fernando C.'], ['Eppstein David'], ['Hutter Marcus'], ['Li Ming'], ['Vitanyi Paul'], ['Gao Qiong'], ['Wolf David R.'], ['Walther Markus'], ['Guergachi A.'], ['Hess Michael'], [\"Aliod Diego Moll'a\"], ['Horrock Ian'], ['Tobies Stephan'], ['Sattler Ulrike'], ['Broeker Norbert'], ['Horrocks Ian'], ['Tokuda Naoyuki'], ['Chen Liang'], ['Gacs Peter'], ['Tromp John'], ['Skourikhine A. N.'], ['Karttunen Lauri'], ['Beesley Kenneth R.'], ['Atserias Jordi'], ['Civit Montse'], ['Castellon Irene'], ['Rigau German'], ['Gerdemann Dale'], ['van Noord Gertjan'], ['Shriberg E.'], ['Stolcke A.'], ['Hakkani-Tur D.'], ['Tur G.'], ['Ratnaparkhi Adwait'], ['Taylor P.'], ['Jurafsky D.'], ['Bates R.'], ['Ries K.'], ['Coccaro N.'], ['Rayner Manny'], ['Hockey Beth Ann'], ['James Frankie'], ['Henderson John C.'], ['Brill Eric'], ['Moses Yoram'], ['Philip Ninan Sajeeth'], ['Joseph K. Babu'], ['Nemenman Ilya'], ['Martins J. F.'], ['Dente J. A.'], ['Pires A. J.'], ['Mendes R. Vilela'], ['Padro L.'], ['Rigau G.'], ['Daude J.'], ['Graff David'], ['Bird Steven'], ['Buneman Peter'], ['Tan Wang-Chiew'], ['Day David'], ['Garofolo John'], ['Laprun Christophe'], ['Henderson John'], ['Zavrel Jakub'], ['Daelemans Walter'], ['Penn Gerald'], ['Marquez Lluis'], ['Escudero Gerard'], ['Watson Richard'], ['Watson Bruce'], ['Mihov Stoyan'], ['Daciuk Jan'], ['Amandi Analia'], ['Zunino Alejandro'], ['Benhamou Frederic'], ['Goualard Frederic'], ['Languenou Eric'], ['Christie Marc'], ['Riezler Stefan'], ['Kuhn Jonas'], ['Johnson Mark'], ['Prescher Detlef'], ['Bond Francis'], ['Ogura Kentaro'], ['Uchino Hajime'], ['Utiyama Masao'], ['Ozaku Hiromi'], ['Ma Qing'], ['Uchimoto Kiyotaka'], ['Murata Masaki'], ['Isahara Hitoshi'], ['Yamamoto Atsumu'], ['Geman Stuart'], ['Canon Stephen'], ['Chi Zhiyi'], ['Charniak Eugene'], ['Roark Brian'], ['Osborne Miles'], ['Dale Robert'], ['Androutsopoulos Ion'], ['Punyakanok Vasin'], ['Roth Dan'], ['Muñoz Marcia'], ['Zimak Dav'], ['Ciaramita Massimiliano'], ['Goerz Guenther'], ['Klarner Martin'], ['Spilker Joerg'], ['De Pauw Guy'], ['Gillis Steven'], ['Hoste Veronique'], ['Sang Erik Tjong Kim'], ['Resnik Philip'], ['Melamed I. Dan'], ['Even-Zohar Yair'], ['Buchholz Sabine'], ['Sang Erik F. Tjong Kim'], ['Provost Foster'], ['Fawcett Tom'], ['Sarkar Anoop'], ['Zeman Daniel'], ['Soklakov Andrei N.'], ['Straccia Umberto'], ['Liberman Mark'], ['Brass Stefan'], ['Przymusinski Teodor C.'], ['Dix Juergen'], ['Kempe Andre'], ['Kalai Adam'], ['Blum Avrim'], ['Wasserman Hal'], ['Brill E.'], ['Mangu L.'], ['Kohavi Ron'], ['Schmidhuber Juergen'], ['Demoen Bart'], ['Blockeel Hendrik'], ['Jacobs Nico'], ['De Raedt Luc'], ['Turner Hudson'], ['Kao Ming-Yang'], ['Csuros Miklos'], ['Van Eynde Frank'], ['Verdoolaege Sven'], ['Schelkens Ness'], ['De Schreye Danny'], ['Kosala Raymond'], ['Ramon Jan'], ['De Mot Emmanuel'], ['Pelov Nikolay'], ['Elworthy David'], ['Berstel Jean'], ['Boasson Luc'], [\"Sima'an Khalil\"], ['Fujii Atsushi'], ['Ishikawa Tetsuya'], ['Subrahmanian VS'], ['Ozcan Fatma'], ['Chater Nick'], ['Lonc Zbigniew'], ['Lam Tak-Wah'], ['Ting Hing-Fung'], ['Sung Wing-Kin'], ['Albro Daniel'], ['Belz Anja'], ['Eisner Jason'], ['Pedersen Ted'], ['Friedman Nir'], ['Kanzaki Kyoko'], ['Kaelbling Leslie'], ['Meuleau Nicolas'], ['Peshkin Leonid'], ['Webber Bonnie'], ['Stone Matthew'], ['Palmer Martha'], ['Doran Christine'], ['Bleam Tonia'], ['Ngai Grace'], ['Florian Radu'], ['Yarowsky David'], ['Schaerf Andrea'], ['van Zaanen Menno'], ['Montemurro Marcelo A.'], ['Buccafurri Francesco'], ['Kim Kee-Eung'], ['Kaelbling Leslie Pack'], ['Simons Gary'], ['Mukherjee Sayan'], ['Kwee Ivo'], ['Dupuy Sylvain'], ['Legendre Vincent'], ['Egges Arjan'], ['Nugues Pierre'], ['Fu Li Min'], ['Jaulin Luc'], ['Ratschan Stefan'], ['Bern Marshall'], ['van Hoeve W. J.'], ['Padró L.'], ['Daudé J.'], ['Christiansen Henning'], ['Krymolowski Yuval'], ['Ando Rie Kubota'], ['Dimopoulos Yannis'], ['Wolfengagen Viacheslav'], ['van Emden M. H.'], ['Rudova Hana'], ['Truszczynski Deborah East. Miroslaw'], ['McKeown Kathleen R.'], ['Kan Min-Yen'], ['Klavans Judith L.'], ['Hammerton James'], ['Dejean Herve'], ['Cancedda Nicola'], ['Nerbonne John'], ['Etalle Sandro'], ['Gabbrielli Maurizio'], ['Meo Maria Chiara'], ['Riloff Ellen'], ['Light Marc'], ['Anand Brianne Brown Pranav'], ['Breck Eric'], ['Mann Gideon S.'], ['Verdejo Felisa'], ['Fernandez-Amoros David'], ['Gonzalo Julio'], ['Granvilliers Laurent'], ['Mahajan Milind'], ['Simon H.'], ['Zha H.'], ['Ding C.'], ['He X.'], ['Gu M.'], ['Tompits H.'], ['Sabbatini G.'], ['Fink M.'], ['Eiter T.'], ['Goodman Joshua'], ['Constable John'], ['Aoyama Hideaki'], ['Padro Lluis'], ['Vaillant Pascal'], ['Carreras Xavier'], ['Knott Alistair'], ['Joshi Aravind'], ['Zanette Damian H.'], ['Sebastiani Fabrizio'], ['Paquet Sebastien'], ['Struyf Jan'], ['Rossi Gianfranco'], ['Dovier Agostino'], ['Pontelli Enrico'], ['Xu Peng'], ['Huang Chu-Ren'], ['Ramer Arthur'], ['Hoffmann Achim'], ['Tyszkiewicz Jerzy'], ['Venkataraman Anand'], ['Ma Bin'], ['Chen Xin'], ['Li Xin'], ['Schiex Thomas'], ['Cooper Martin'], ['Tennenholtz Moshe'], ['Vanhoof Koen'], ['Goethals Bart'], ['Wets Geert'], ['Swinnen Gilbert'], ['Brijs Tom'], ['Bussche Jan Van den'], ['Geerts Floris']]\n"
     ]
    }
   ],
   "source": [
    "print(authors_pd.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|█████████████████████████████████████████| 425/425 [00:06<00:00, 69.13it/s]\n",
      "100%|████████████████████████████████████████| 319/319 [00:00<00:00, 514.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.6591], device='cuda:0', dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|█████████████████████████████████████████| 425/425 [00:06<00:00, 69.07it/s]\n",
      "100%|████████████████████████████████████████| 319/319 [00:00<00:00, 517.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0595], device='cuda:0', dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|█████████████████████████████████████████| 425/425 [00:06<00:00, 69.82it/s]\n",
      "100%|████████████████████████████████████████| 319/319 [00:00<00:00, 515.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0023], device='cuda:0', dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|█████████████████████████████████████████| 425/425 [00:06<00:00, 69.57it/s]\n",
      "100%|████████████████████████████████████████| 319/319 [00:00<00:00, 514.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0007], device='cuda:0', dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|█████████████████████████████████████████| 425/425 [00:06<00:00, 69.06it/s]\n",
      "100%|████████████████████████████████████████| 319/319 [00:00<00:00, 516.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0003], device='cuda:0', dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|█████████████████████████████████████████| 425/425 [00:06<00:00, 69.36it/s]\n",
      "100%|████████████████████████████████████████| 319/319 [00:00<00:00, 517.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0001], device='cuda:0', dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|█████████████████████████████████████████| 425/425 [00:06<00:00, 69.05it/s]\n",
      "100%|████████████████████████████████████████| 319/319 [00:00<00:00, 523.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.2389e-05], device='cuda:0', dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|█████████████████████████████████████████| 425/425 [00:06<00:00, 69.88it/s]\n",
      "100%|████████████████████████████████████████| 319/319 [00:00<00:00, 488.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.1721e-05], device='cuda:0', dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|█████████████████████████████████████████| 425/425 [00:06<00:00, 67.45it/s]\n",
      "100%|████████████████████████████████████████| 319/319 [00:00<00:00, 493.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.6564e-05], device='cuda:0', dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|█████████████████████████████████████████| 425/425 [00:06<00:00, 67.20it/s]\n",
      "100%|████████████████████████████████████████| 319/319 [00:00<00:00, 498.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8.8221e-06], device='cuda:0', dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "AS, PS = wmr.algo(2001, papers_pd_list, authors_pd_list, 0.70, 0.60, 0.5, 1.0, 0.85, 1e-6, AUTHOR_PAST_YEARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
